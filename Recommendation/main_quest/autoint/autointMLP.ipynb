{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a5b1403-d11a-4e9c-910d-c1e27ea92f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer, MaxPooling2D, Conv2D, Dropout, Lambda, Dense, Flatten, Activation, Input, Embedding, BatchNormalization\n",
    "from tensorflow.keras.initializers import glorot_normal, Zeros, TruncatedNormal\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import BinaryAccuracy\n",
    "\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from collections import defaultdict\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bd87f1b-6326-4dc7-9478-0a0bb2efcf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì„ë² ë”© ë ˆì´ì–´\n",
    "class FeaturesEmbedding(Layer):  \n",
    "    '''\n",
    "    ì„ë² ë”© ë ˆì´ì–´ì…ë‹ˆë‹¤. \n",
    "    - ë§Œì•½ í”¼ì²˜(feature) 3ê°œê°€ ê°ê° 10ê°œ, 20ê°œ, 30ê°œì˜ ê³ ìœ ê°’ì„ ê°€ì§„ë‹¤ë©´ feature_dimsëŠ” [10, 20, 30] í˜•íƒœë¥¼ ë„ê²Œ ë©ë‹ˆë‹¤.\n",
    "    - ì „ì²´ ì„ë² ë”©ì„ í•´ì•¼ í•  ê°œìˆ˜ëŠ” 10+20+30 = 60ì´ë¯€ë¡œ '60 x ì„ë² ë”©_ì°¨ì›_í¬ê¸°'ì˜ í–‰ë ¬ì´ ìƒì„±ë˜ê²Œ ë©ë‹ˆë‹¤.\n",
    "    '''\n",
    "    def __init__(self, field_dims, embed_dim, **kwargs):\n",
    "        super(FeaturesEmbedding, self).__init__(**kwargs)\n",
    "        self.total_dim = sum(field_dims)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.offsets = np.array((0, *np.cumsum(field_dims)[:-1]), dtype=np.longlong)\n",
    "        self.embedding = tf.keras.layers.Embedding(input_dim=self.total_dim, output_dim=self.embed_dim)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # ì„ë² ë”©ì„ ë¹Œë“œí•˜ê³  ì´ˆê¸°í™”í•©ë‹ˆë‹¤.\n",
    "        self.embedding.build(input_shape)\n",
    "        self.embedding.set_weights([tf.keras.initializers.GlorotUniform()(shape=self.embedding.weights[0].shape)])\n",
    "\n",
    "    def call(self, x):\n",
    "        # ë“¤ì–´ì˜¨ ì…ë ¥ì˜ ì„ë² ë”©ì„ ê°€ì ¸ë‹ˆë‹¤.\n",
    "        x = x + tf.constant(self.offsets)\n",
    "        return self.embedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4b5788c-ae60-4c6b-bff2-b351cd83e818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP ë ˆì´ì–´\n",
    "class MultiLayerPerceptron(Layer):  \n",
    "    '''\n",
    "    DNN ë ˆì´ì–´ì…ë‹ˆë‹¤.\n",
    "    - Tensorflow Kerasì—ì„œëŠ” Dense ë ˆì´ì–´ë¥¼ ìŒ“ì•„ì˜¬ë¦° êµ¬ì¡°ì…ë‹ˆë‹¤.\n",
    "    - í•„ìš”ì— ë”°ë¼ ë°°ì¹˜ ì •ê·œí™”ë„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "    '''\n",
    "    def __init__(self, input_dim, hidden_units, activation='relu', l2_reg=0, dropout_rate=0, use_bn=False, init_std=0.0001, output_layer=True):\n",
    "        super(MultiLayerPerceptron, self).__init__()\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.use_bn = use_bn\n",
    "        hidden_units = [input_dim] + list(hidden_units)\n",
    "        if output_layer:\n",
    "            hidden_units += [1]\n",
    "        # Dense layerë¥¼ ìŒ“ì•„ì˜¬ë¦½ë‹ˆë‹¤.\n",
    "        self.linears = [Dense(units, activation=None, kernel_initializer=tf.random_normal_initializer(stddev=init_std),\n",
    "                              kernel_regularizer=tf.keras.regularizers.l2(l2_reg)) for units in hidden_units[1:]]\n",
    "        # í™œì„±í™” í•¨ìˆ˜ë¥¼ ì„¸íŒ…í•©ë‹ˆë‹¤.\n",
    "        self.activation = tf.keras.layers.Activation(activation)\n",
    "        # í•„ìš”í•˜ë‹¤ë©´ ë°°ì¹˜ì •ê·œí™”ë„ ì§„í–‰í•©ë‹ˆë‹¤.\n",
    "        if self.use_bn:\n",
    "            self.bn = [BatchNormalization() for _ in hidden_units[1:]]\n",
    "        self.dropout = Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = inputs\n",
    "        for i in range(len(self.linears)):\n",
    "            # input dataê°€ ë“¤ì–´ì˜¤ë©´ layerë¥¼ ëŒë©´ì„œ ë²¡í„° ê°’ì„ ê°€ì ¸ì˜¤ê²Œ ë©ë‹ˆë‹¤.\n",
    "            x = self.linears[i](x)\n",
    "            if self.use_bn:\n",
    "                x = self.bn[i](x, training=training)\n",
    "            # ê° layerë§ˆë‹¤ ë‚˜ì˜¨ ë²¡í„° ê°’ì— í™œì„±í™” í•¨ìˆ˜ì™€ dropoutì„ ì ìš©ì‹œì¼œ ë¹„ì„ í˜•ì„± êµ¬ì¡°ì™€ ê³¼ì í•©ì„ ë°©ì§€í•©ë‹ˆë‹¤.\n",
    "            x = self.activation(x)\n",
    "            x = self.dropout(x, training=training)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4e560de-7c25-4ace-a02c-9490a6a70cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë©€í‹°í—¤ë“œ ì–´í…ì…˜ ë ˆì´ì–´\n",
    "class MultiHeadSelfAttention(Layer):  \n",
    "    '''\n",
    "    ë©€í‹° í—¤ë“œ ì…€í”„ ì–´í…ì…˜ ë ˆì´ì–´ì…ë‹ˆë‹¤.\n",
    "    - ìœ„ì— ì‘ì„±í•œ ìˆ˜ì‹ê³¼ ê°™ì´ ë™ì‘ë©ë‹ˆë‹¤.\n",
    "    - í•„ìš”ì— ë”°ë¼ ì”ì°¨ ì—°ê²°(residual connection)ë„ ì§„í–‰í•©ë‹ˆë‹¤.\n",
    "    '''\n",
    "    def __init__(self, att_embedding_size=8, head_num=2, use_res=True, scaling=False, seed=1024, **kwargs):\n",
    "        if head_num <= 0:\n",
    "            raise ValueError('head_num must be a int > 0')\n",
    "        self.att_embedding_size = att_embedding_size\n",
    "        self.head_num = head_num\n",
    "        self.use_res = use_res\n",
    "        self.seed = seed\n",
    "        self.scaling = scaling\n",
    "        super(MultiHeadSelfAttention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if len(input_shape) != 3:\n",
    "            raise ValueError(\n",
    "                \"Unexpected inputs dimensions %d, expect to be 3 dimensions\" % (len(input_shape)))\n",
    "        embedding_size = int(input_shape[-1])\n",
    "        # ì¿¼ë¦¬ì— í•´ë‹¹í•˜ëŠ” ë§¤íŠ¸ë¦­ìŠ¤ì…ë‹ˆë‹¤. \n",
    "        self.W_Query = self.add_weight(name='query', shape=[embedding_size, self.att_embedding_size * self.head_num],\n",
    "                                       dtype=tf.float32,\n",
    "                                       initializer=TruncatedNormal(seed=self.seed))\n",
    "        # í‚¤ì— í•´ë‹¹ë˜ëŠ” ë§¤íŠ¸ë¦­ìŠ¤ì…ë‹ˆë‹¤.\n",
    "        self.W_key = self.add_weight(name='key', shape=[embedding_size, self.att_embedding_size * self.head_num],\n",
    "                                     dtype=tf.float32,\n",
    "                                     initializer=TruncatedNormal(seed=self.seed + 1))\n",
    "        # ê°’(value)ì— í•´ë‹¹ë˜ëŠ” ë§¤íŠ¸ë¦­ìŠ¤ì…ë‹ˆë‹¤.\n",
    "        self.W_Value = self.add_weight(name='value', shape=[embedding_size, self.att_embedding_size * self.head_num],\n",
    "                                       dtype=tf.float32,\n",
    "                                       initializer=TruncatedNormal(seed=self.seed + 2))\n",
    "        # í•„ìš”í•˜ë‹¤ë©´ ì”ì°¨ ì—°ê²°ë„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "        if self.use_res:\n",
    "            self.W_Res = self.add_weight(name='res', shape=[embedding_size, self.att_embedding_size * self.head_num],\n",
    "                                         dtype=tf.float32,\n",
    "                                         initializer=TruncatedNormal(seed=self.seed))\n",
    "\n",
    "        super(MultiHeadSelfAttention, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        if K.ndim(inputs) != 3:\n",
    "            raise ValueError(\"Unexpected inputs dimensions %d, expect to be 3 dimensions\" % (K.ndim(inputs)))\n",
    "        \n",
    "        # ì…ë ¥ì´ ë“¤ì–´ì˜¤ë©´ ì¿¼ë¦¬, í‚¤, ê°’(value)ì— ë§¤ì¹­ë˜ì–´ ê°ê°ì˜ ê°’ì„ ê°€ì§€ê³  ì˜µë‹ˆë‹¤.\n",
    "        querys = tf.tensordot(inputs, self.W_Query, axes=(-1, 0))  \n",
    "        keys = tf.tensordot(inputs, self.W_key, axes=(-1, 0))\n",
    "        values = tf.tensordot(inputs, self.W_Value, axes=(-1, 0))\n",
    "\n",
    "        # í—¤ë“œ ê°œìˆ˜ì— ë”°ë¼ ë°ì´í„°ë¥¼ ë¶„ë¦¬í•´ì¤ë‹ˆë‹¤.\n",
    "        querys = tf.stack(tf.split(querys, self.head_num, axis=2))\n",
    "        keys = tf.stack(tf.split(keys, self.head_num, axis=2))\n",
    "        values = tf.stack(tf.split(values, self.head_num, axis=2))\n",
    "        \n",
    "        # ì¿¼ë¦¬ì™€ í‚¤ë¥¼ ë¨¼ì € ê³±í•´ì¤ë‹ˆë‹¤. ìœ„ ì´ë¯¸ì§€ì˜ ì‹ (5)ì™€ ê°™ìŠµë‹ˆë‹¤.\n",
    "        inner_product = tf.matmul(querys, keys, transpose_b=True)\n",
    "        if self.scaling:\n",
    "            inner_product /= self.att_embedding_size ** 0.5\n",
    "        self.normalized_att_scores =  tf.nn.softmax(inner_product)\n",
    "        \n",
    "        # ì¿¼ë¦¬ì™€ í‚¤ì—ì„œ ë‚˜ì˜¨ ì–´í…ì…˜ ê°’ì„ ê°’(value)ì— ê³±í•´ì¤ë‹ˆë‹¤. ì‹ (6)ê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
    "        result = tf.matmul(self.normalized_att_scores, values)\n",
    "        # ì‹ (7)ê³¼ ê°™ì´ ìª¼ê°œì–´ì§„ ë©€í…Œ í—¤ë“œë¥¼ ëª¨ì•„ì¤ë‹ˆë‹¤.\n",
    "        result = tf.concat(tf.split(result, self.head_num, ), axis=-1)\n",
    "        result = tf.squeeze(result, axis=0) \n",
    "\n",
    "        if self.use_res:\n",
    "            result += tf.tensordot(inputs, self.W_Res, axes=(-1, 0))\n",
    "        result = tf.nn.relu(result)\n",
    "        \n",
    "        # ê·¸ ê²°ê³¼ ê°’ì„ ë¦¬í„´í•©ë‹ˆë‹¤.\n",
    "\n",
    "        return result\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "\n",
    "        return (None, input_shape[1], self.att_embedding_size * self.head_num)\n",
    "\n",
    "    def get_config(self, ):\n",
    "        config = {'att_embedding_size': self.att_embedding_size, 'head_num': self.head_num, 'use_res': self.use_res,'seed': self.seed}\n",
    "        base_config = super(MultiHeadSelfAttention, self).get_config()\n",
    "        base_config.update(config)\n",
    "        return base_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60205d28-f441-42a1-98e5-3e2679019afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoIntMLP(Layer): \n",
    "    def __init__(self, field_dims, embedding_size, att_layer_num=3, att_head_num=2, att_res=True, dnn_hidden_units=(32, 32), dnn_activation='relu',\n",
    "                 l2_reg_dnn=0, l2_reg_embedding=1e-5, dnn_use_bn=False, dnn_dropout=0.4, init_std=0.0001):\n",
    "        super(AutoIntMLP, self).__init__()\n",
    "        self.embedding = FeaturesEmbedding(field_dims, embedding_size)\n",
    "        self.num_fields = len(field_dims)\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "        self.final_layer = Dense(1, use_bias=False, kernel_initializer=tf.random_normal_initializer(stddev=init_std))\n",
    "        \n",
    "        self.dnn = tf.keras.Sequential()\n",
    "        for units in dnn_hidden_units:\n",
    "            self.dnn.add(Dense(units, activation=dnn_activation,\n",
    "                               kernel_regularizer=tf.keras.regularizers.l2(l2_reg_dnn),\n",
    "                               kernel_initializer=tf.random_normal_initializer(stddev=init_std)))\n",
    "            if dnn_use_bn:\n",
    "                self.dnn.add(BatchNormalization())\n",
    "            self.dnn.add(Activation(dnn_activation))\n",
    "            if dnn_dropout > 0:\n",
    "                self.dnn.add(Dropout(dnn_dropout))\n",
    "        self.dnn.add(Dense(1, kernel_initializer=tf.random_normal_initializer(stddev=init_std)))\n",
    "\n",
    "        self.int_layers = [MultiHeadSelfAttention(att_embedding_size=embedding_size, head_num=att_head_num, use_res=att_res) for _ in range(att_layer_num)]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        embed_x = self.embedding(inputs)\n",
    "        dnn_embed = tf.reshape(embed_x, shape=(-1, self.embedding_size * self.num_fields))\n",
    "\n",
    "        att_input = embed_x\n",
    "        for layer in self.int_layers:\n",
    "            att_input = layer(att_input)\n",
    "\n",
    "        att_output = Flatten()(att_input)\n",
    "        att_output = self.final_layer(att_output)\n",
    "        \n",
    "        dnn_output = self.dnn(dnn_embed)\n",
    "        y_pred = tf.keras.activations.sigmoid(att_output + dnn_output)\n",
    "        \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0fb029a-12d1-44c6-99ba-beb548346fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_DCG(ranklist, y_true):\n",
    "    dcg = 0.0\n",
    "    for i in range(len(ranklist)):\n",
    "        item = ranklist[i]\n",
    "        if item in y_true:\n",
    "            dcg += 1.0 / math.log(i + 2)\n",
    "    return  dcg\n",
    "\n",
    "def get_IDCG(ranklist, y_true):\n",
    "    idcg = 0.0\n",
    "    i = 0\n",
    "    for item in y_true:\n",
    "        if item in ranklist:\n",
    "            idcg += 1.0 / math.log(i + 2)\n",
    "            i += 1\n",
    "    return idcg\n",
    "\n",
    "def get_NDCG(ranklist, y_true):\n",
    "    '''NDCG í‰ê°€ ì§€í‘œ'''\n",
    "    ranklist = np.array(ranklist).astype(int)\n",
    "    y_true = np.array(y_true).astype(int)\n",
    "    dcg = get_DCG(ranklist, y_true)\n",
    "    idcg = get_IDCG(y_true, y_true)\n",
    "    if idcg == 0:\n",
    "        return 0\n",
    "    return round( (dcg / idcg), 5)\n",
    "\n",
    "def get_hit_rate(ranklist, y_true):\n",
    "    '''hitrate í‰ê°€ ì§€í‘œ'''\n",
    "    c = 0\n",
    "    for y in y_true:\n",
    "        if y in ranklist:\n",
    "            c += 1\n",
    "    return round( c / len(y_true), 5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f1a7eb9-51ea-4bcd-ac1f-9108dd5539e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_df):\n",
    "    '''ëª¨ë¸ í…ŒìŠ¤íŠ¸'''\n",
    "    user_pred_info = defaultdict(list)\n",
    "    total_rows = len(test_df)\n",
    "    for i in range(0, total_rows, batch_size):\n",
    "        features = test_df.iloc[i:i + batch_size, :-1].values\n",
    "        y_pred = model.predict(features, verbose=False)\n",
    "        for feature, p in zip(features, y_pred):\n",
    "            u_i = feature[:2]\n",
    "            user_pred_info[int(u_i[0])].append((int(u_i[1]), float(p.item())))\n",
    "    return user_pred_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0052593d-e509-4372-902d-b46ec7363efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_path = os.path.abspath(os.getcwd())\n",
    "data_dir_nm = 'data'\n",
    "movielens_dir_nm = 'ml-1m'\n",
    "model_dir_nm = 'model'\n",
    "data_path = f\"{project_path}/{data_dir_nm}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27ee8409-13af-485d-b874-991e7f8d08c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000209, 15)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>movie_decade</th>\n",
       "      <th>movie_year</th>\n",
       "      <th>rating_year</th>\n",
       "      <th>rating_month</th>\n",
       "      <th>rating_decade</th>\n",
       "      <th>genre1</th>\n",
       "      <th>genre2</th>\n",
       "      <th>genre3</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>occupation</th>\n",
       "      <th>zip</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>1970's</td>\n",
       "      <td>1975</td>\n",
       "      <td>2000</td>\n",
       "      <td>12</td>\n",
       "      <td>2000's</td>\n",
       "      <td>Drama</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>1990's</td>\n",
       "      <td>1996</td>\n",
       "      <td>2000</td>\n",
       "      <td>12</td>\n",
       "      <td>2000's</td>\n",
       "      <td>Animation</td>\n",
       "      <td>Children's</td>\n",
       "      <td>Musical</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>1960's</td>\n",
       "      <td>1964</td>\n",
       "      <td>2000</td>\n",
       "      <td>12</td>\n",
       "      <td>2000's</td>\n",
       "      <td>Musical</td>\n",
       "      <td>Romance</td>\n",
       "      <td>no</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>2000's</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "      <td>12</td>\n",
       "      <td>2000's</td>\n",
       "      <td>Drama</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>1990's</td>\n",
       "      <td>1998</td>\n",
       "      <td>2001</td>\n",
       "      <td>1</td>\n",
       "      <td>2000's</td>\n",
       "      <td>Animation</td>\n",
       "      <td>Children's</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  user_id movie_id movie_decade movie_year rating_year rating_month  \\\n",
       "0       1     1193       1970's       1975        2000           12   \n",
       "1       1      661       1990's       1996        2000           12   \n",
       "2       1      914       1960's       1964        2000           12   \n",
       "3       1     3408       2000's       2000        2000           12   \n",
       "4       1     2355       1990's       1998        2001            1   \n",
       "\n",
       "  rating_decade     genre1      genre2   genre3 gender age occupation    zip  \\\n",
       "0        2000's      Drama          no       no      F   1         10  48067   \n",
       "1        2000's  Animation  Children's  Musical      F   1         10  48067   \n",
       "2        2000's    Musical     Romance       no      F   1         10  48067   \n",
       "3        2000's      Drama          no       no      F   1         10  48067   \n",
       "4        2000's  Animation  Children's   Comedy      F   1         10  48067   \n",
       "\n",
       "  label  \n",
       "0     1  \n",
       "1     0  \n",
       "2     0  \n",
       "3     1  \n",
       "4     1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "# csv ë°ì´í„°ì´ë¯€ë¡œ read_csvë¡œ ê°€ì ¸ì˜µë‹ˆë‹¤.\n",
    "movielens_rcmm = pd.read_csv(f\"../../../../data/autoint/ml-1m/movielens_rcmm_v2.csv\", dtype=str)\n",
    "print(movielens_rcmm.shape)\n",
    "movielens_rcmm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "690c85d9-9b77-466a-aa1c-6cf51bf45970",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoders = {col: LabelEncoder() for col in movielens_rcmm.columns[:-1]} # labelì€ ì œì™¸\n",
    "\n",
    "for col, le in label_encoders.items():\n",
    "    movielens_rcmm[col] = le.fit_transform(movielens_rcmm[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86640c83-e5cf-488b-815d-db4820114faa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>movie_decade</th>\n",
       "      <th>movie_year</th>\n",
       "      <th>rating_year</th>\n",
       "      <th>rating_month</th>\n",
       "      <th>rating_decade</th>\n",
       "      <th>genre1</th>\n",
       "      <th>genre2</th>\n",
       "      <th>genre3</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>occupation</th>\n",
       "      <th>zip</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>189</td>\n",
       "      <td>6</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1588</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3374</td>\n",
       "      <td>8</td>\n",
       "      <td>76</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1588</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>3615</td>\n",
       "      <td>5</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1588</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2503</td>\n",
       "      <td>9</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1588</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1374</td>\n",
       "      <td>8</td>\n",
       "      <td>78</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1588</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  movie_id  movie_decade  movie_year  rating_year  rating_month  \\\n",
       "0        0       189             6          55            0             3   \n",
       "1        0      3374             8          76            0             3   \n",
       "2        0      3615             5          44            0             3   \n",
       "3        0      2503             9          80            0             3   \n",
       "4        0      1374             8          78            1             0   \n",
       "\n",
       "   rating_decade  genre1  genre2  genre3  gender  age  occupation   zip label  \n",
       "0              0       7      17      15       0    0           2  1588     1  \n",
       "1              0       2       2       8       0    0           2  1588     0  \n",
       "2              0      11      12      15       0    0           2  1588     0  \n",
       "3              0       7      17      15       0    0           2  1588     1  \n",
       "4              0       2       2       2       0    0           2  1588     1  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movielens_rcmm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b28799a-c086-4c5c-a7ee-6e1ce457b0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "movielens_rcmm['label'] = movielens_rcmm['label'].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20073fe4-9501-43d4-bc6b-f9b0c31aff16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. í•™ìŠµ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ë°ì´í„°ë¡œ ë¶„ë¦¬, 0.2 ì •ë„ë¡œ ë¶„ë¦¬\n",
    "train_df, test_df = train_test_split(movielens_rcmm, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79ef4c5c-09b4-4316-bd8c-5c3fa9bfbf0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 800167 entries, 416292 to 121958\n",
      "Data columns (total 15 columns):\n",
      " #   Column         Non-Null Count   Dtype  \n",
      "---  ------         --------------   -----  \n",
      " 0   user_id        800167 non-null  int64  \n",
      " 1   movie_id       800167 non-null  int64  \n",
      " 2   movie_decade   800167 non-null  int64  \n",
      " 3   movie_year     800167 non-null  int64  \n",
      " 4   rating_year    800167 non-null  int64  \n",
      " 5   rating_month   800167 non-null  int64  \n",
      " 6   rating_decade  800167 non-null  int64  \n",
      " 7   genre1         800167 non-null  int64  \n",
      " 8   genre2         800167 non-null  int64  \n",
      " 9   genre3         800167 non-null  int64  \n",
      " 10  gender         800167 non-null  int64  \n",
      " 11  age            800167 non-null  int64  \n",
      " 12  occupation     800167 non-null  int64  \n",
      " 13  zip            800167 non-null  int64  \n",
      " 14  label          800167 non-null  float32\n",
      "dtypes: float32(1), int64(14)\n",
      "memory usage: 94.6 MB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a98a4e3c-a49a-4d8c-ada1-817734db2c40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6040, 3706,   10,   81,    4,   12,    1,   18,   18,   16,    2,\n",
       "          7,   21, 3439])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# í•„ìš” ì»¬ëŸ¼ë“¤ê³¼ ë ˆì´ë¸” ì •ì˜\n",
    "# í•„ë“œì˜ ê° ê³ ìœ  ê°œìˆ˜ë¥¼ ì •ì˜í•˜ëŠ” field_dimsë¥¼ ì •ì˜í•©ë‹ˆë‹¤. ì´ëŠ”  ì„ë² ë”© ë•Œ í™œìš©ë©ë‹ˆë‹¤. \n",
    "u_i_feature = ['user_id', 'movie_id']\n",
    "meta_features = ['movie_decade', 'movie_year', 'rating_year', 'rating_month', 'rating_decade', 'genre1','genre2', 'genre3', 'gender', 'age', 'occupation', 'zip']\n",
    "label = 'label'\n",
    "field_dims = np.max(movielens_rcmm[u_i_feature + meta_features].astype(np.int64).values, axis=0) + 1\n",
    "field_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca6ef474-9f2d-40ef-92f9-7045b70c97e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì—í¬í¬, í•™ìŠµë¥ , ë“œë¡­ì•„ì›ƒ, ë°°ì¹˜ì‚¬ì´ì¦ˆ, ì„ë² ë”© í¬ê¸° ë“± ì •ì˜\n",
    "epochs=5\n",
    "learning_rate= 0.0001\n",
    "dropout= 0.4\n",
    "batch_size = 2048\n",
    "embed_dim= 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7db428b-e070-41fe-b320-541fbfa623e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoIntMLPModel(Model):\n",
    "    def __init__(self, field_dims, embedding_size, att_layer_num=3, att_head_num=2,\n",
    "                 att_res=True, dnn_hidden_units=(32, 32), dnn_activation='relu',\n",
    "                 l2_reg_dnn=0, l2_reg_embedding=1e-5, dnn_use_bn=False,\n",
    "                 dnn_dropout=0.4, init_std=0.0001):\n",
    "        super(AutoIntMLPModel, self).__init__()\n",
    "        self.autoInt_layer = AutoIntMLP(\n",
    "            field_dims=field_dims,\n",
    "            embedding_size=embedding_size,\n",
    "            att_layer_num=att_layer_num,\n",
    "            att_head_num=att_head_num,\n",
    "            att_res=att_res,\n",
    "            dnn_hidden_units=dnn_hidden_units,\n",
    "            dnn_activation=dnn_activation,\n",
    "            l2_reg_dnn=l2_reg_dnn,\n",
    "            l2_reg_embedding=l2_reg_embedding,\n",
    "            dnn_use_bn=dnn_use_bn,\n",
    "            dnn_dropout=dnn_dropout,\n",
    "            init_std=init_std\n",
    "        )\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        return self.autoInt_layer(inputs, training=training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74df10d9-1f34-445b-9860-5fe954630420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ ì •ì˜\n",
    "autoIntMLP_model = AutoIntMLPModel(\n",
    "    field_dims=field_dims,\n",
    "    embedding_size=embed_dim,\n",
    "    att_layer_num=3,\n",
    "    att_head_num=2,\n",
    "    att_res=True,\n",
    "    dnn_hidden_units=(32, 32),               # ì¶”ê°€: DNN ì€ë‹‰ì¸µ êµ¬ì¡°\n",
    "    dnn_activation='relu',                  # ì¶”ê°€: í™œì„±í™” í•¨ìˆ˜\n",
    "    l2_reg_dnn=0,\n",
    "    l2_reg_embedding=1e-5,\n",
    "    dnn_use_bn=False,\n",
    "    dnn_dropout=dropout,\n",
    "    init_std=0.0001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a3939140-c79c-460b-a64c-784b7b8d8979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì˜µí‹°ë§ˆì´ì €, ì˜¤ì°¨í•¨ìˆ˜ ì •ì˜\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "loss_fn = BinaryCrossentropy(from_logits=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9bea2ad7-263b-4868-8147-fe6fdc5f5613",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoIntMLP_model.compile(optimizer=optimizer, loss=loss_fn, metrics=['binary_crossentropy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c946800-0196-47d8-a37c-2ca6f29f55b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 12ms/step - binary_crossentropy: 0.6774 - loss: 0.6774 - val_binary_crossentropy: 0.6481 - val_loss: 0.6481\n",
      "Epoch 2/5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - binary_crossentropy: 0.6176 - loss: 0.6176 - val_binary_crossentropy: 0.5917 - val_loss: 0.5917\n",
      "Epoch 3/5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - binary_crossentropy: 0.5666 - loss: 0.5666 - val_binary_crossentropy: 0.5497 - val_loss: 0.5497\n",
      "Epoch 4/5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - binary_crossentropy: 0.5414 - loss: 0.5414 - val_binary_crossentropy: 0.5429 - val_loss: 0.5429\n",
      "Epoch 5/5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - binary_crossentropy: 0.5362 - loss: 0.5362 - val_binary_crossentropy: 0.5408 - val_loss: 0.5408\n"
     ]
    }
   ],
   "source": [
    "history = autoIntMLP_model.fit(train_df[u_i_feature + meta_features], train_df[label], epochs=epochs, batch_size=batch_size, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1580e261-a1e4-49d5-aac8-82bd6b5474f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6038/6038 [00:00<00:00, 145201.69it/s]\n"
     ]
    }
   ],
   "source": [
    "# ì‚¬ìš©ìì—ê²Œ ì˜ˆì¸¡ëœ ì •ë³´ë¥¼ ì €ì¥í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ \n",
    "user_pred_info = {}\n",
    "# top10ê°œ\n",
    "top = 10\n",
    "# í…ŒìŠ¤íŠ¸ ê°’ì„ ê°€ì§€ê³  ì˜µë‹ˆë‹¤. \n",
    "mymodel_user_pred_info = test_model(autoIntMLP_model, test_df)\n",
    "# ì‚¬ìš©ìë§ˆë‹¤ ëŒë©´ì„œ ì˜ˆì¸¡ ë°ì´í„° ì¤‘ ê°€ì¥ ë†’ì€ top 10ë§Œ ê°€ì ¸ì˜µë‹ˆë‹¤. \n",
    "for user, data_info in tqdm(mymodel_user_pred_info.items(), total=len(mymodel_user_pred_info), position=0, leave=True):\n",
    "    ranklist = sorted(data_info, key=lambda s : s[1], reverse=True)[:top]\n",
    "    ranklist = list(dict.fromkeys([r[0] for r in ranklist]))\n",
    "    user_pred_info[str(user)] = ranklist\n",
    "# ì›ë³¸ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ labelì´ 1ì¸ ì‚¬ìš©ì ë³„ ì˜í™” ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.\n",
    "test_data = test_df[test_df['label']==1].groupby('user_id')['movie_id'].apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f64635e7-50a1-4113-8a7b-f3d6a2aa7ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5994/5994 [00:00<00:00, 16293.95it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5994/5994 [00:00<00:00, 101486.98it/s]\n"
     ]
    }
   ],
   "source": [
    "mymodel_ndcg_result = {}\n",
    "mymodel_hitrate_result = {}\n",
    "\n",
    "# ëª¨ë¸ ì˜ˆì¸¡ê°’ê³¼ ì›ë³¸ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë¹„êµí•´ì„œ ì–´ëŠì •ë„ ì„±ëŠ¥ì´ ë‚˜ì™”ëŠ”ì§€ NDCGì™€ Hitrateë¥¼ ë¹„êµí•©ë‹ˆë‹¤.\n",
    "\n",
    "# NDCG\n",
    "for user, data_info in tqdm(test_data.items(), total=len(test_data), position=0, leave=True):\n",
    "    mymodel_pred = user_pred_info.get(str(user))\n",
    "\n",
    "    testset = list(set(np.array(data_info).astype(int)))\n",
    "    mymodel_pred = mymodel_pred[:top]\n",
    "\n",
    "    # NDCG ê°’ êµ¬í•˜ê¸°\n",
    "    user_ndcg = get_NDCG(mymodel_pred, testset)\n",
    "\n",
    "    mymodel_ndcg_result[user] = user_ndcg\n",
    "\n",
    "# Hitrate\n",
    "for user, data_info in tqdm(test_data.items(), total=len(test_data), position=0, leave=True):\n",
    "    mymodel_pred = user_pred_info.get(str(user))\n",
    "\n",
    "    testset = list(set(np.array(data_info).astype(int)))\n",
    "    mymodel_pred = mymodel_pred[:top]\n",
    "\n",
    "    # hitrate ê°’ êµ¬í•˜ê¸°\n",
    "    user_hitrate = get_hit_rate(mymodel_pred, testset)\n",
    "\n",
    "    # ì‚¬ìš©ì hitrate ê²°ê³¼ ì €ì¥\n",
    "    mymodel_hitrate_result[user] = user_hitrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b0ba51d4-dc4a-4fde-9448-3414aa643e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " mymodel ndcg :  0.66155\n",
      " mymodel hitrate :  0.6301\n"
     ]
    }
   ],
   "source": [
    "print(\" mymodel ndcg : \", round(np.mean(list(mymodel_ndcg_result.values())), 5))\n",
    "print(\" mymodel hitrate : \", round(np.mean(list(mymodel_hitrate_result.values())), 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fab702cd-b09c-4b9d-863e-dd33ef3a0739",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('autoint/model', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "456e3ff5-22fb-47ac-9e63-ed8e55c946e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoIntMLP_model.save_weights('./model/autoInt_mlp_model_weights_1.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "67e5a77d-986e-4caa-a1e8-675276507897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from autoint_tuner\\random_search_autoint\\tuner0.json\n",
      "\n",
      "ğŸ¯ Best Hyperparameters:\n",
      "Learning rate: 0.0001\n",
      "Dropout: 0.5\n",
      "Embed dim: 32\n",
      "Hidden units: 128\n",
      "Batch size: 1024\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"auto_int_mlp_model_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"auto_int_mlp_model_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                         </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape                </span>â”ƒ<span style=\"font-weight: bold\">         Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ auto_int_mlp_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AutoIntMLP</span>)          â”‚ ?                           â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ auto_int_mlp_1 (\u001b[38;5;33mAutoIntMLP\u001b[0m)          â”‚ ?                           â”‚     \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m704/704\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - binary_crossentropy: 0.6229 - loss: 0.6229 - val_binary_crossentropy: 0.5517 - val_loss: 0.5517\n",
      "Epoch 2/20\n",
      "\u001b[1m704/704\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - binary_crossentropy: 0.5451 - loss: 0.5451 - val_binary_crossentropy: 0.5408 - val_loss: 0.5408\n",
      "Epoch 3/20\n",
      "\u001b[1m704/704\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - binary_crossentropy: 0.5375 - loss: 0.5375 - val_binary_crossentropy: 0.5392 - val_loss: 0.5392\n",
      "Epoch 4/20\n",
      "\u001b[1m704/704\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - binary_crossentropy: 0.5347 - loss: 0.5347 - val_binary_crossentropy: 0.5386 - val_loss: 0.5386\n",
      "Epoch 5/20\n",
      "\u001b[1m704/704\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - binary_crossentropy: 0.5333 - loss: 0.5333 - val_binary_crossentropy: 0.5391 - val_loss: 0.5391\n",
      "Epoch 6/20\n",
      "\u001b[1m704/704\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - binary_crossentropy: 0.5324 - loss: 0.5324 - val_binary_crossentropy: 0.5377 - val_loss: 0.5377\n",
      "Epoch 7/20\n",
      "\u001b[1m704/704\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - binary_crossentropy: 0.5314 - loss: 0.5314 - val_binary_crossentropy: 0.5371 - val_loss: 0.5371\n",
      "Epoch 8/20\n",
      "\u001b[1m704/704\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - binary_crossentropy: 0.5306 - loss: 0.5306 - val_binary_crossentropy: 0.5368 - val_loss: 0.5368\n",
      "Epoch 9/20\n",
      "\u001b[1m704/704\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - binary_crossentropy: 0.5299 - loss: 0.5299 - val_binary_crossentropy: 0.5362 - val_loss: 0.5362\n",
      "Epoch 10/20\n",
      "\u001b[1m704/704\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - binary_crossentropy: 0.5289 - loss: 0.5289 - val_binary_crossentropy: 0.5364 - val_loss: 0.5364\n",
      "Epoch 11/20\n",
      "\u001b[1m704/704\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - binary_crossentropy: 0.5273 - loss: 0.5273 - val_binary_crossentropy: 0.5340 - val_loss: 0.5340\n",
      "Epoch 12/20\n",
      "\u001b[1m704/704\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - binary_crossentropy: 0.5256 - loss: 0.5256 - val_binary_crossentropy: 0.5327 - val_loss: 0.5327\n",
      "Epoch 13/20\n",
      "\u001b[1m704/704\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - binary_crossentropy: 0.5239 - loss: 0.5239 - val_binary_crossentropy: 0.5316 - val_loss: 0.5316\n",
      "Epoch 14/20\n",
      "\u001b[1m704/704\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 12ms/step - binary_crossentropy: 0.5225 - loss: 0.5225 - val_binary_crossentropy: 0.5310 - val_loss: 0.5310\n",
      "Epoch 15/20\n",
      "\u001b[1m704/704\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - binary_crossentropy: 0.5213 - loss: 0.5213 - val_binary_crossentropy: 0.5301 - val_loss: 0.5301\n",
      "Epoch 16/20\n",
      "\u001b[1m704/704\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 12ms/step - binary_crossentropy: 0.5201 - loss: 0.5201 - val_binary_crossentropy: 0.5302 - val_loss: 0.5302\n",
      "Epoch 17/20\n",
      "\u001b[1m704/704\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - binary_crossentropy: 0.5189 - loss: 0.5189 - val_binary_crossentropy: 0.5292 - val_loss: 0.5292\n",
      "Epoch 18/20\n",
      "\u001b[1m704/704\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - binary_crossentropy: 0.5181 - loss: 0.5181 - val_binary_crossentropy: 0.5294 - val_loss: 0.5294\n",
      "Epoch 19/20\n",
      "\u001b[1m704/704\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - binary_crossentropy: 0.5172 - loss: 0.5172 - val_binary_crossentropy: 0.5287 - val_loss: 0.5287\n",
      "Epoch 20/20\n",
      "\u001b[1m704/704\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - binary_crossentropy: 0.5166 - loss: 0.5166 - val_binary_crossentropy: 0.5289 - val_loss: 0.5289\n",
      "Restoring model weights from the end of the best epoch: 19.\n",
      "Results summary\n",
      "Results in autoint_tuner\\random_search_autoint\n",
      "Showing 10 best trials\n",
      "Objective(name=\"val_loss\", direction=\"min\")\n",
      "\n",
      "Trial 00 summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0001\n",
      "dropout: 0.5\n",
      "embed_dim: 32\n",
      "hidden_units: 128\n",
      "batch_size: 1024\n",
      "Score: 0.5144122242927551\n",
      "\n",
      "Trial 02 summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.001\n",
      "dropout: 0.5\n",
      "embed_dim: 16\n",
      "hidden_units: 64\n",
      "batch_size: 2048\n",
      "Score: 0.5151307582855225\n",
      "\n",
      "Trial 01 summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.001\n",
      "dropout: 0.3\n",
      "embed_dim: 16\n",
      "hidden_units: 64\n",
      "batch_size: 512\n",
      "Score: 0.5154075622558594\n",
      "\n",
      "Trial 06 summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.001\n",
      "dropout: 0.4\n",
      "embed_dim: 16\n",
      "hidden_units: 128\n",
      "batch_size: 2048\n",
      "Score: 0.5160378813743591\n",
      "\n",
      "Trial 05 summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0001\n",
      "dropout: 0.4\n",
      "embed_dim: 32\n",
      "hidden_units: 64\n",
      "batch_size: 2048\n",
      "Score: 0.5168991684913635\n",
      "\n",
      "Trial 03 summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0001\n",
      "dropout: 0.5\n",
      "embed_dim: 16\n",
      "hidden_units: 128\n",
      "batch_size: 512\n",
      "Score: 0.5170079469680786\n",
      "\n",
      "Trial 07 summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0005\n",
      "dropout: 0.4\n",
      "embed_dim: 8\n",
      "hidden_units: 64\n",
      "batch_size: 512\n",
      "Score: 0.5173991918563843\n",
      "\n",
      "Trial 04 summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.001\n",
      "dropout: 0.3\n",
      "embed_dim: 32\n",
      "hidden_units: 32\n",
      "batch_size: 512\n",
      "Score: 0.5179414749145508\n",
      "\n",
      "Trial 08 summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.001\n",
      "dropout: 0.4\n",
      "embed_dim: 32\n",
      "hidden_units: 32\n",
      "batch_size: 512\n",
      "Score: 0.5192090272903442\n",
      "\n",
      "Trial 09 summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0001\n",
      "dropout: 0.5\n",
      "embed_dim: 8\n",
      "hidden_units: 32\n",
      "batch_size: 2048\n",
      "Score: 0.5237553715705872\n"
     ]
    }
   ],
   "source": [
    "import keras_tuner as kt\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "\n",
    "# ==========================\n",
    "# 1ï¸âƒ£ ëª¨ë¸ ë¹Œë” í•¨ìˆ˜ ì •ì˜\n",
    "# ==========================\n",
    "def build_autoint_model(hp):\n",
    "    # íŠœë„ˆì—ì„œ ì œì–´í•  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì •ì˜\n",
    "    learning_rate = hp.Choice('learning_rate', [1e-3, 5e-4, 1e-4])\n",
    "    dropout = hp.Choice('dropout', [0.3, 0.4, 0.5])\n",
    "    embed_dim = hp.Choice('embed_dim', [8, 16, 32])\n",
    "    hidden_units = hp.Choice('hidden_units', [32, 64, 128])\n",
    "    batch_size = hp.Choice('batch_size', [512, 1024, 2048])   # âœ… ì—¬ê¸°ì— batch_size ì¶”ê°€\n",
    "\n",
    "    model = AutoIntMLPModel(\n",
    "        field_dims=field_dims,\n",
    "        embedding_size=embed_dim,\n",
    "        att_layer_num=3,\n",
    "        att_head_num=2,\n",
    "        att_res=True,\n",
    "        dnn_hidden_units=(hidden_units, hidden_units),\n",
    "        dnn_activation='relu',\n",
    "        l2_reg_dnn=0,\n",
    "        l2_reg_embedding=1e-5,\n",
    "        dnn_use_bn=False,\n",
    "        dnn_dropout=dropout,\n",
    "        init_std=0.0001\n",
    "    )\n",
    "\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    loss_fn = BinaryCrossentropy(from_logits=False)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=loss_fn, metrics=['binary_crossentropy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# 2ï¸âƒ£ RandomSearch ì •ì˜\n",
    "# ==========================\n",
    "tuner = kt.RandomSearch(\n",
    "    build_autoint_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=10,               # ì‹œë„í•  ì¡°í•© ìˆ˜\n",
    "    executions_per_trial=1,      # ê° ì¡°í•©ë‹¹ ì‹¤í–‰ íšŸìˆ˜\n",
    "    directory='autoint_tuner',\n",
    "    project_name='random_search_autoint'\n",
    ")\n",
    "\n",
    "# ==========================\n",
    "# 3ï¸âƒ£ EarlyStopping ì½œë°±\n",
    "# ==========================\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=2,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ==========================\n",
    "# 4ï¸âƒ£ íƒìƒ‰ ì‹¤í–‰\n",
    "# ==========================\n",
    "tuner.search(\n",
    "    x=train_df[u_i_feature + meta_features],\n",
    "    y=train_df[label],\n",
    "    epochs=20,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ==========================\n",
    "# 5ï¸âƒ£ ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¶œë ¥\n",
    "# ==========================\n",
    "best_hp = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(\"\\nğŸ¯ Best Hyperparameters:\")\n",
    "print(f\"Learning rate: {best_hp.get('learning_rate')}\")\n",
    "print(f\"Dropout: {best_hp.get('dropout')}\")\n",
    "print(f\"Embed dim: {best_hp.get('embed_dim')}\")\n",
    "print(f\"Hidden units: {best_hp.get('hidden_units')}\")\n",
    "print(f\"Batch size: {best_hp.get('batch_size')}\")\n",
    "\n",
    "# ==========================\n",
    "# 6ï¸âƒ£ ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ ìƒˆ ëª¨ë¸ ë¹Œë“œ\n",
    "# ==========================\n",
    "best_model = build_autoint_model(best_hp)\n",
    "\n",
    "# ëª¨ë¸ ë¹Œë“œ (ì…ë ¥ shape ì§€ì • â€” ì»¤ìŠ¤í…€ ëª¨ë¸ì˜ ê²½ìš° í•„ìš”)\n",
    "input_dim = train_df[u_i_feature + meta_features].shape[1]\n",
    "\n",
    "# ëª¨ë¸ êµ¬ì¡° ì¶œë ¥\n",
    "best_model.summary()\n",
    "\n",
    "# ==========================\n",
    "# 7ï¸âƒ£ ìµœì  ëª¨ë¸ë¡œ ìµœì¢… í•™ìŠµ\n",
    "# ==========================\n",
    "history = best_model.fit(\n",
    "    train_df[u_i_feature + meta_features],\n",
    "    train_df[label],\n",
    "    epochs=20,\n",
    "    batch_size=best_hp.get('batch_size'),\n",
    "    validation_split=0.1,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ==========================\n",
    "# 8ï¸âƒ£ íŠœë‹ ìš”ì•½ ë³´ê¸°\n",
    "# ==========================\n",
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "16c60c8c-d691-4ae9-881d-405f292e2710",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6038/6038 [00:00<00:00, 149463.28it/s]\n"
     ]
    }
   ],
   "source": [
    "# ì‚¬ìš©ìì—ê²Œ ì˜ˆì¸¡ëœ ì •ë³´ë¥¼ ì €ì¥í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ \n",
    "user_pred_info = {}\n",
    "# top10ê°œ\n",
    "top = 10\n",
    "# í…ŒìŠ¤íŠ¸ ê°’ì„ ê°€ì§€ê³  ì˜µë‹ˆë‹¤. \n",
    "mymodel_user_pred_info = test_model(best_model, test_df)\n",
    "# ì‚¬ìš©ìë§ˆë‹¤ ëŒë©´ì„œ ì˜ˆì¸¡ ë°ì´í„° ì¤‘ ê°€ì¥ ë†’ì€ top 10ë§Œ ê°€ì ¸ì˜µë‹ˆë‹¤. \n",
    "for user, data_info in tqdm(mymodel_user_pred_info.items(), total=len(mymodel_user_pred_info), position=0, leave=True):\n",
    "    ranklist = sorted(data_info, key=lambda s : s[1], reverse=True)[:top]\n",
    "    ranklist = list(dict.fromkeys([r[0] for r in ranklist]))\n",
    "    user_pred_info[str(user)] = ranklist\n",
    "# ì›ë³¸ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ labelì´ 1ì¸ ì‚¬ìš©ì ë³„ ì˜í™” ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.\n",
    "test_data = test_df[test_df['label']==1].groupby('user_id')['movie_id'].apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "79486844-a5c8-4a36-8872-330b0391dd81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5994/5994 [00:00<00:00, 16028.91it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5994/5994 [00:00<00:00, 99189.85it/s]\n"
     ]
    }
   ],
   "source": [
    "mymodel_ndcg_result = {}\n",
    "mymodel_hitrate_result = {}\n",
    "\n",
    "# ëª¨ë¸ ì˜ˆì¸¡ê°’ê³¼ ì›ë³¸ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë¹„êµí•´ì„œ ì–´ëŠì •ë„ ì„±ëŠ¥ì´ ë‚˜ì™”ëŠ”ì§€ NDCGì™€ Hitrateë¥¼ ë¹„êµí•©ë‹ˆë‹¤.\n",
    "\n",
    "# NDCG\n",
    "for user, data_info in tqdm(test_data.items(), total=len(test_data), position=0, leave=True):\n",
    "    mymodel_pred = user_pred_info.get(str(user))\n",
    "\n",
    "    testset = list(set(np.array(data_info).astype(int)))\n",
    "    mymodel_pred = mymodel_pred[:top]\n",
    "\n",
    "    # NDCG ê°’ êµ¬í•˜ê¸°\n",
    "    user_ndcg = get_NDCG(mymodel_pred, testset)\n",
    "\n",
    "    mymodel_ndcg_result[user] = user_ndcg\n",
    "\n",
    "# Hitrate\n",
    "for user, data_info in tqdm(test_data.items(), total=len(test_data), position=0, leave=True):\n",
    "    mymodel_pred = user_pred_info.get(str(user))\n",
    "\n",
    "    testset = list(set(np.array(data_info).astype(int)))\n",
    "    mymodel_pred = mymodel_pred[:top]\n",
    "\n",
    "    # hitrate ê°’ êµ¬í•˜ê¸°\n",
    "    user_hitrate = get_hit_rate(mymodel_pred, testset)\n",
    "\n",
    "    # ì‚¬ìš©ì hitrate ê²°ê³¼ ì €ì¥\n",
    "    mymodel_hitrate_result[user] = user_hitrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "14533247-f7f1-4f24-8bac-9f0b1ef14b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " mymodel ndcg :  0.66357\n",
      " mymodel hitrate :  0.63115\n"
     ]
    }
   ],
   "source": [
    "print(\" mymodel ndcg : \", round(np.mean(list(mymodel_ndcg_result.values())), 5))\n",
    "print(\" mymodel hitrate : \", round(np.mean(list(mymodel_hitrate_result.values())), 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9ef7156a-d1d6-4ca8-af0e-e3b94694e2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.save_weights('./model/autoInt_mlp_model_weights_2.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a65a880-ab56-494e-85aa-0d2e7c50ec21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
