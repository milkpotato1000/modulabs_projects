{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c15943b5-8aa7-48fb-ac8b-ded5bdd89570",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from tensorflow.keras.datasets import reuters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d9c98d-dd76-4eca-964d-177d0ca15383",
   "metadata": {},
   "source": [
    "# 단어장 개수별 ML 모델 성능 비교 (Accuracy / F1-score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5acabe1-0f26-48f1-a538-01418964a755",
   "metadata": {},
   "source": [
    "## 1. 데이터 로드 & 디코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "30691326-2075-475d-a1c8-489803fe69b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train_idx, y_train), (x_test_idx, y_test) = reuters.load_data(num_words=None, test_split=0.2)\n",
    "word_index = reuters.get_word_index(path='reuters_word_index.json')\n",
    "index_to_word = {index+3: word for word, index in word_index.items()}\n",
    "for index, token in enumerate(('<pad>', '<sos>', '<unk>')):\n",
    "  index_to_word[index]=token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d2069a5e-d581-4d15-977e-8d83adf0e77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train/x_test 디코딩\n",
    "x_train = [' '.join([index_to_word.get(i, '?') for i in seq]) for seq in x_train_idx]\n",
    "x_test  = [' '.join([index_to_word.get(i, '?') for i in seq]) for seq in x_test_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb522096-9140-4dde-8c4d-2ce25a8bc5e7",
   "metadata": {},
   "source": [
    "## 2. ML모델 및 실험할 vocab_size 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "535653c7-f2d7-4305-a6aa-c89d2e8e2e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 정의\n",
    "models = {\n",
    "    'LogisticRegression': LogisticRegression(max_iter=1000),\n",
    "    'SVM': SVC(probability=True),\n",
    "    'RandomForest': RandomForestClassifier(),\n",
    "    'XGBoost': GradientBoostingClassifier(),\n",
    "    'NaiveBayes': ComplementNB(),\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'LightGBM': LGBMClassifier(verbose=-1),\n",
    "    'DecisionTree': DecisionTreeClassifier()\n",
    "}\n",
    "\n",
    "# 단어 수 정의\n",
    "vocab_sizes = [10000, 5000, None]  # None = all words\n",
    "\n",
    "# 결과 저장 DataFrame 정의\n",
    "columns = ['Vocabulary Size', 'Model', 'Accuracy', 'F1-Score']\n",
    "df_results = pd.DataFrame(columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e483b16f-1ad1-4f48-b17e-bd9007770327",
   "metadata": {},
   "source": [
    "## 3.훈련 및 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a8dd6d81-cfc5-474d-b0ea-e18943951a17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********Vocab Size = 10000********\n",
      "LogisticRegression: [Accuracy] = 0.7969723953695459 / [F1_score] = 0.7742748415216105\n",
      "SVM: [Accuracy] = 0.8063223508459484 / [F1_score] = 0.7884500873330293\n",
      "RandomForest: [Accuracy] = 0.748886910062333 / [F1_score] = 0.7245028630808907\n",
      "XGBoost: [Accuracy] = 0.7702582368655387 / [F1_score] = 0.7667588606246989\n",
      "NaiveBayes: [Accuracy] = 0.7724844167408726 / [F1_score] = 0.7472902633131123\n",
      "KNN: [Accuracy] = 0.7867319679430098 / [F1_score] = 0.7789670680335509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\miniconda3\\envs\\tfgpuenv\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM: [Accuracy] = 0.21861086375779162 / [F1_score] = 0.2632495484476679\n",
      "DecisionTree: [Accuracy] = 0.6985752448797863 / [F1_score] = 0.6938593680755918\n",
      "********Vocab Size = 5000********\n",
      "LogisticRegression: [Accuracy] = 0.798753339269813 / [F1_score] = 0.7765773185291976\n",
      "SVM: [Accuracy] = 0.8081032947462155 / [F1_score] = 0.7915424120120957\n",
      "RandomForest: [Accuracy] = 0.7666963490650045 / [F1_score] = 0.745721113898975\n",
      "XGBoost: [Accuracy] = 0.7658058771148709 / [F1_score] = 0.761096816674096\n",
      "NaiveBayes: [Accuracy] = 0.7689225289403384 / [F1_score] = 0.7432270502020317\n",
      "KNN: [Accuracy] = 0.7894033837934105 / [F1_score] = 0.7802436439764467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\miniconda3\\envs\\tfgpuenv\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM: [Accuracy] = 0.20525378450578807 / [F1_score] = 0.19645327370471216\n",
      "DecisionTree: [Accuracy] = 0.6918967052537845 / [F1_score] = 0.6872725838522098\n",
      "********All words********\n",
      "LogisticRegression: [Accuracy] = 0.7916295636687445 / [F1_score] = 0.7670211296471304\n",
      "SVM: [Accuracy] = 0.7996438112199465 / [F1_score] = 0.7806611171251367\n",
      "RandomForest: [Accuracy] = 0.7395369545859305 / [F1_score] = 0.7125347080338018\n",
      "XGBoost: [Accuracy] = 0.7604630454140695 / [F1_score] = 0.7586929131324669\n",
      "NaiveBayes: [Accuracy] = 0.7649154051647373 / [F1_score] = 0.7346534179503126\n",
      "KNN: [Accuracy] = 0.7720391807658059 / [F1_score] = 0.76393321267862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\miniconda3\\envs\\tfgpuenv\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM: [Accuracy] = 0.1923419412288513 / [F1_score] = 0.1781183849277486\n",
      "DecisionTree: [Accuracy] = 0.7030276046304541 / [F1_score] = 0.69793005166038\n"
     ]
    }
   ],
   "source": [
    "results_list = []\n",
    "\n",
    "for vocab in vocab_sizes:\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=vocab)\n",
    "    X_train_tfidf = tfidf_vectorizer.fit_transform(x_train)\n",
    "    X_test_tfidf = tfidf_vectorizer.transform(x_test)\n",
    "\n",
    "    if vocab is not None:\n",
    "        print(f'{\"*\"*8}Vocab Size = {vocab}{\"*\"*8}')\n",
    "    else:\n",
    "        print(f'{\"*\"*8}All words{\"*\"*8}')\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        model.fit(X_train_tfidf, y_train)\n",
    "        y_pred = model.predict(X_test_tfidf)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "        # 평가 결과 출력\n",
    "        print(f'{model_name}: [Accuracy] = {acc} / [F1_score] = {f1}')\n",
    "        \n",
    "        # 평가 결과 dict 형태로 저장\n",
    "        results_list.append({\n",
    "            'Vocabulary Size': vocab if vocab is not None else 'All words',\n",
    "            'Model': model_name,\n",
    "            'Accuracy': acc,\n",
    "            'F1-Score': f1\n",
    "        })\n",
    "\n",
    "# 반복 후 한 번에 DataFrame 생성\n",
    "df_results = pd.DataFrame(results_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ff74d318-12e8-4c3b-b9b4-f9a7150dcfc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Vocabulary Size</th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1-Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5000</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.808103</td>\n",
       "      <td>0.791542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10000</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.806322</td>\n",
       "      <td>0.788450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>All words</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.799644</td>\n",
       "      <td>0.780661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5000</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.789403</td>\n",
       "      <td>0.780244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10000</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.786732</td>\n",
       "      <td>0.778967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5000</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.798753</td>\n",
       "      <td>0.776577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.796972</td>\n",
       "      <td>0.774275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>All words</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.791630</td>\n",
       "      <td>0.767021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10000</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.770258</td>\n",
       "      <td>0.766759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>All words</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.772039</td>\n",
       "      <td>0.763933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5000</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.765806</td>\n",
       "      <td>0.761097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>All words</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.760463</td>\n",
       "      <td>0.758693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10000</td>\n",
       "      <td>NaiveBayes</td>\n",
       "      <td>0.772484</td>\n",
       "      <td>0.747290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5000</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.766696</td>\n",
       "      <td>0.745721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5000</td>\n",
       "      <td>NaiveBayes</td>\n",
       "      <td>0.768923</td>\n",
       "      <td>0.743227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>All words</td>\n",
       "      <td>NaiveBayes</td>\n",
       "      <td>0.764915</td>\n",
       "      <td>0.734653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10000</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.748887</td>\n",
       "      <td>0.724503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>All words</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.739537</td>\n",
       "      <td>0.712535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>All words</td>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>0.703028</td>\n",
       "      <td>0.697930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10000</td>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>0.698575</td>\n",
       "      <td>0.693859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5000</td>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>0.691897</td>\n",
       "      <td>0.687273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10000</td>\n",
       "      <td>LightGBM</td>\n",
       "      <td>0.218611</td>\n",
       "      <td>0.263250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5000</td>\n",
       "      <td>LightGBM</td>\n",
       "      <td>0.205254</td>\n",
       "      <td>0.196453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>All words</td>\n",
       "      <td>LightGBM</td>\n",
       "      <td>0.192342</td>\n",
       "      <td>0.178118</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Vocabulary Size               Model  Accuracy  F1-Score\n",
       "9             5000                 SVM  0.808103  0.791542\n",
       "1            10000                 SVM  0.806322  0.788450\n",
       "17       All words                 SVM  0.799644  0.780661\n",
       "13            5000                 KNN  0.789403  0.780244\n",
       "5            10000                 KNN  0.786732  0.778967\n",
       "8             5000  LogisticRegression  0.798753  0.776577\n",
       "0            10000  LogisticRegression  0.796972  0.774275\n",
       "16       All words  LogisticRegression  0.791630  0.767021\n",
       "3            10000             XGBoost  0.770258  0.766759\n",
       "21       All words                 KNN  0.772039  0.763933\n",
       "11            5000             XGBoost  0.765806  0.761097\n",
       "19       All words             XGBoost  0.760463  0.758693\n",
       "4            10000          NaiveBayes  0.772484  0.747290\n",
       "10            5000        RandomForest  0.766696  0.745721\n",
       "12            5000          NaiveBayes  0.768923  0.743227\n",
       "20       All words          NaiveBayes  0.764915  0.734653\n",
       "2            10000        RandomForest  0.748887  0.724503\n",
       "18       All words        RandomForest  0.739537  0.712535\n",
       "23       All words        DecisionTree  0.703028  0.697930\n",
       "7            10000        DecisionTree  0.698575  0.693859\n",
       "15            5000        DecisionTree  0.691897  0.687273\n",
       "6            10000            LightGBM  0.218611  0.263250\n",
       "14            5000            LightGBM  0.205254  0.196453\n",
       "22       All words            LightGBM  0.192342  0.178118"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results.sort_values(by=['F1-Score', 'Accuracy'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938af92e-4702-4866-b39c-1dd64b32fe52",
   "metadata": {},
   "source": [
    "# 벡터화 방법별 ML/DL 모델 성능 비교 (Accuracy / F1-score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "73ea04d3-1a7d-4647-94a4-5ffa7f2bd694",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, Embedding, Flatten\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.datasets import reuters\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2603894-b405-474b-a8ce-620705031358",
   "metadata": {},
   "source": [
    "## 1. 인풋 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fed4b8a6-ae13-485d-91df-94e323e20f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('stangeland', 0.8456367254257202), ('bow', 0.8367922306060791), ('lai', 0.8266149759292603), ('read', 0.8237302303314209), ('glenn', 0.8221088647842407), ('iowa', 0.8213066458702087), ('myers', 0.820145845413208), ('sydney', 0.8192616701126099), ('cooperative', 0.8173328042030334), ('missouri', 0.8158978819847107)]\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF 벡터화\n",
    "tfidf5000_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = tfidf5000_vectorizer.fit_transform(x_train)\n",
    "X_test_tfidf = tfidf5000_vectorizer.transform(x_test)\n",
    "\n",
    "# W2V 방식\n",
    "x_train_tokenized = [sentence.split() for sentence in x_train]\n",
    "x_test_tokenized = [sentence.split() for sentence in x_test]\n",
    "\n",
    "model = Word2Vec(sentences = x_train_tokenized, vector_size = 512, window = 5, min_count = 5, workers = 4, sg = 0)\n",
    "model_result = model.wv.most_similar('man')\n",
    "print(model_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "95692609-17c8-47d8-8dfd-cf2102dfcb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습된 Word2Vec 모델\n",
    "w2v_model = model\n",
    "\n",
    "# 각 문장을 벡터화 시키는 코드\n",
    "def vectorize_sentence(sentence, model, max_len):\n",
    "    vecs = []\n",
    "    for word in sentence:\n",
    "        if word in model.wv:\n",
    "            vecs.append(model.wv[word])\n",
    "        else:\n",
    "            vecs.append(np.zeros(model.vector_size))\n",
    "    # Padding\n",
    "    if len(vecs) < max_len:\n",
    "        vecs += [np.zeros(model.vector_size)] * (max_len - len(vecs))\n",
    "    else:\n",
    "        vecs = vecs[:max_len]\n",
    "    return np.array(vecs)\n",
    "\n",
    "x_train_w2v = np.array([vectorize_sentence(s, w2v_model, max_len=100) for s in x_train_tokenized])\n",
    "x_test_w2v = np.array([vectorize_sentence(s, w2v_model, max_len=100) for s in x_test_tokenized])\n",
    "\n",
    "num_classes = len(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4d4513e3-ebb0-47c5-ae91-2761c03818cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8982, 100, 512)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_w2v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802020da-726b-4e8a-9c24-5f2832892098",
   "metadata": {},
   "source": [
    "## 3. 훈련 및 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "079f635a-f40b-4e54-9e59-1b227c976b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a136e3be-57e5-4c20-b389-6988ab66e417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1️. SVM\n",
    "# TF-IDF\n",
    "svm_tfidf = SVC(probability=True)\n",
    "svm_tfidf.fit(X_train_tfidf, y_train)\n",
    "y_pred = svm_tfidf.predict(X_test_tfidf)\n",
    "results_list.append({\n",
    "    'Vectorization': 'TF-IDF',\n",
    "    'Model': 'SVM',\n",
    "    'Accuracy': accuracy_score(y_test, y_pred),\n",
    "    'F1-Score': f1_score(y_test, y_pred, average='weighted')\n",
    "})\n",
    "\n",
    "# Word2Vec (문장 평균 벡터)\n",
    "def avg_vector(sentence_vec):\n",
    "    return np.mean(sentence_vec, axis=0)\n",
    "\n",
    "X_train_w2v_avg = np.array([avg_vector(s) for s in x_train_w2v])\n",
    "X_test_w2v_avg = np.array([avg_vector(s) for s in x_test_w2v])\n",
    "\n",
    "svm_w2v = SVC(probability=True)\n",
    "svm_w2v.fit(X_train_w2v_avg, y_train)\n",
    "y_pred = svm_w2v.predict(X_test_w2v_avg)\n",
    "results_list.append({\n",
    "    'Vectorization': 'Word2Vec',\n",
    "    'Model': 'SVM',\n",
    "    'Accuracy': accuracy_score(y_test, y_pred),\n",
    "    'F1-Score': f1_score(y_test, y_pred, average='weighted')\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "dc09e4dc-4f76-4f76-8f9b-613f077b7d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "281/281 [==============================] - 1s 2ms/step - loss: 1.5791 - accuracy: 0.6355\n",
      "Epoch 2/20\n",
      "281/281 [==============================] - 1s 2ms/step - loss: 0.8049 - accuracy: 0.8145\n",
      "Epoch 3/20\n",
      "281/281 [==============================] - 1s 2ms/step - loss: 0.5158 - accuracy: 0.8779\n",
      "Epoch 4/20\n",
      "281/281 [==============================] - 1s 2ms/step - loss: 0.3355 - accuracy: 0.9127\n",
      "Epoch 5/20\n",
      "281/281 [==============================] - 1s 2ms/step - loss: 0.2511 - accuracy: 0.9340\n",
      "Epoch 6/20\n",
      "281/281 [==============================] - 1s 2ms/step - loss: 0.1992 - accuracy: 0.9422\n",
      "Epoch 7/20\n",
      "281/281 [==============================] - 1s 2ms/step - loss: 0.1825 - accuracy: 0.9480\n",
      "Epoch 8/20\n",
      "281/281 [==============================] - 1s 2ms/step - loss: 0.1681 - accuracy: 0.9473\n",
      "Epoch 9/20\n",
      "281/281 [==============================] - 1s 2ms/step - loss: 0.1472 - accuracy: 0.9539\n",
      "Epoch 10/20\n",
      "281/281 [==============================] - 1s 2ms/step - loss: 0.1476 - accuracy: 0.9520\n",
      "Epoch 11/20\n",
      "281/281 [==============================] - 1s 2ms/step - loss: 0.1359 - accuracy: 0.9531\n",
      "Epoch 12/20\n",
      "281/281 [==============================] - 1s 2ms/step - loss: 0.1263 - accuracy: 0.9539\n",
      "Epoch 13/20\n",
      "281/281 [==============================] - 1s 2ms/step - loss: 0.1228 - accuracy: 0.9555\n",
      "Epoch 14/20\n",
      "281/281 [==============================] - 1s 2ms/step - loss: 0.1179 - accuracy: 0.9554\n",
      "Epoch 15/20\n",
      "281/281 [==============================] - 1s 2ms/step - loss: 0.1146 - accuracy: 0.9567\n",
      "Epoch 16/20\n",
      "281/281 [==============================] - 1s 2ms/step - loss: 0.1134 - accuracy: 0.9570\n",
      "Epoch 17/20\n",
      "281/281 [==============================] - 1s 2ms/step - loss: 0.1047 - accuracy: 0.9565\n",
      "Epoch 18/20\n",
      "281/281 [==============================] - 1s 2ms/step - loss: 0.1034 - accuracy: 0.9575\n",
      "Epoch 19/20\n",
      "281/281 [==============================] - 1s 2ms/step - loss: 0.0997 - accuracy: 0.9568\n",
      "Epoch 20/20\n",
      "281/281 [==============================] - 1s 2ms/step - loss: 0.0974 - accuracy: 0.9564\n",
      "71/71 [==============================] - 0s 882us/step\n",
      "Epoch 1/20\n",
      "281/281 [==============================] - 1s 2ms/step - loss: 1.5814 - accuracy: 0.6487\n",
      "Epoch 2/20\n",
      "281/281 [==============================] - 1s 2ms/step - loss: 1.2423 - accuracy: 0.7074\n",
      "Epoch 3/20\n",
      "281/281 [==============================] - 1s 2ms/step - loss: 1.1560 - accuracy: 0.7238\n",
      "Epoch 4/20\n",
      "281/281 [==============================] - 1s 2ms/step - loss: 1.1052 - accuracy: 0.7344\n",
      "Epoch 5/20\n",
      "281/281 [==============================] - 1s 2ms/step - loss: 1.0619 - accuracy: 0.7379\n",
      "Epoch 6/20\n",
      "281/281 [==============================] - 1s 2ms/step - loss: 1.0452 - accuracy: 0.7435\n",
      "Epoch 7/20\n",
      "281/281 [==============================] - 1s 2ms/step - loss: 1.0180 - accuracy: 0.7477\n",
      "Epoch 8/20\n",
      "281/281 [==============================] - 1s 2ms/step - loss: 1.0057 - accuracy: 0.7522\n",
      "Epoch 9/20\n",
      "281/281 [==============================] - 1s 2ms/step - loss: 0.9737 - accuracy: 0.7580\n",
      "Epoch 10/20\n",
      "281/281 [==============================] - 1s 2ms/step - loss: 0.9640 - accuracy: 0.7612\n",
      "Epoch 11/20\n",
      "281/281 [==============================] - 1s 2ms/step - loss: 0.9586 - accuracy: 0.7591\n",
      "Epoch 12/20\n",
      "281/281 [==============================] - 1s 2ms/step - loss: 0.9364 - accuracy: 0.7666\n",
      "Epoch 13/20\n",
      "281/281 [==============================] - 1s 2ms/step - loss: 0.9219 - accuracy: 0.7697\n",
      "Epoch 14/20\n",
      "281/281 [==============================] - 1s 2ms/step - loss: 0.9054 - accuracy: 0.7722\n",
      "Epoch 15/20\n",
      "281/281 [==============================] - 1s 2ms/step - loss: 0.8904 - accuracy: 0.7735\n",
      "Epoch 16/20\n",
      "281/281 [==============================] - 1s 2ms/step - loss: 0.8827 - accuracy: 0.7742\n",
      "Epoch 17/20\n",
      "281/281 [==============================] - 1s 2ms/step - loss: 0.8685 - accuracy: 0.7756\n",
      "Epoch 18/20\n",
      "281/281 [==============================] - 1s 2ms/step - loss: 0.8618 - accuracy: 0.7773\n",
      "Epoch 19/20\n",
      "281/281 [==============================] - 1s 2ms/step - loss: 0.8524 - accuracy: 0.7768\n",
      "Epoch 20/20\n",
      "281/281 [==============================] - 1s 2ms/step - loss: 0.8346 - accuracy: 0.7829\n",
      "71/71 [==============================] - 0s 777us/step\n"
     ]
    }
   ],
   "source": [
    "# 2️. Dense NN\n",
    "# TF-IDF\n",
    "dense_tfidf = Sequential([\n",
    "    Dense(512, activation='relu', input_shape=(X_train_tfidf.shape[1],)),\n",
    "    Dropout(0.3),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(len(np.unique(y_train)), activation='softmax')\n",
    "])\n",
    "dense_tfidf.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "dense_tfidf.fit(X_train_tfidf.toarray(), y_train, epochs=20, batch_size=32, verbose=1)\n",
    "y_pred = np.argmax(dense_tfidf.predict(X_test_tfidf.toarray()), axis=1)\n",
    "results_list.append({\n",
    "    'Vectorization': 'TF-IDF',\n",
    "    'Model': 'Dense NN',\n",
    "    'Accuracy': accuracy_score(y_test, y_pred),\n",
    "    'F1-Score': f1_score(y_test, y_pred, average='weighted')\n",
    "})\n",
    "\n",
    "# Word2Vec\n",
    "dense_w2v = Sequential([\n",
    "    Dense(512, activation='relu', input_shape=(X_train_w2v_avg.shape[1],)),\n",
    "    Dropout(0.3),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(len(np.unique(y_train)), activation='softmax')\n",
    "])\n",
    "dense_w2v.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "dense_w2v.fit(X_train_w2v_avg, y_train, epochs=20, batch_size=32, verbose=1)\n",
    "y_pred = np.argmax(dense_w2v.predict(X_test_w2v_avg), axis=1)\n",
    "results_list.append({\n",
    "    'Vectorization': 'Word2Vec',\n",
    "    'Model': 'Dense NN',\n",
    "    'Accuracy': accuracy_score(y_test, y_pred),\n",
    "    'F1-Score': f1_score(y_test, y_pred, average='weighted')\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0d83ce85-383a-4775-bd7f-e428330affb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71/71 [==============================] - 0s 930us/step\n",
      "71/71 [==============================] - 1s 8ms/step\n"
     ]
    }
   ],
   "source": [
    "# 3️. RNN\n",
    "# TF-IDF 기반 RNN\n",
    "# 단어 인덱스로 변환\n",
    "tokenizer = tfidf_vectorizer  # 이미 fit된 TF-IDF vectorizer 사용\n",
    "X_train_seq_tfidf = X_train_tfidf.toarray()\n",
    "X_test_seq_tfidf = X_test_tfidf.toarray()\n",
    "\n",
    "rnn_tfidf = Sequential([\n",
    "    Dense(512, activation='relu', input_shape=(X_train_seq_tfidf.shape[1],)),\n",
    "    Dropout(0.3),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(len(np.unique(y_train)), activation='softmax')\n",
    "])\n",
    "rnn_tfidf.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "rnn_tfidf.fit(X_train_seq_tfidf, y_train, epochs=20, batch_size=32, verbose=1)\n",
    "y_pred = np.argmax(rnn_tfidf.predict(X_test_seq_tfidf), axis=1)\n",
    "results_list.append({\n",
    "    'Vectorization': 'TF-IDF',\n",
    "    'Model': 'RNN',\n",
    "    'Accuracy': accuracy_score(y_test, y_pred),\n",
    "    'F1-Score': f1_score(y_test, y_pred, average='weighted')\n",
    "})\n",
    "\n",
    "# Word2Vec 기반 RNN\n",
    "# 이미 x_train_w2v / x_test_w2v 준비됨 (shape: [num_samples, max_len, embedding_dim])\n",
    "rnn_w2v = Sequential([\n",
    "    LSTM(512, input_shape=(x_train_w2v.shape[1], x_train_w2v.shape[2])),\n",
    "    Dropout(0.3),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(len(np.unique(y_train)), activation='softmax')\n",
    "])\n",
    "rnn_w2v.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "rnn_w2v.fit(x_train_w2v, y_train, epochs=20, batch_size=32, verbose=1)\n",
    "y_pred = np.argmax(rnn_w2v.predict(x_test_w2v), axis=1)\n",
    "results_list.append({\n",
    "    'Vectorization': 'Word2Vec',\n",
    "    'Model': 'RNN',\n",
    "    'Accuracy': accuracy_score(y_test, y_pred),\n",
    "    'F1-Score': f1_score(y_test, y_pred, average='weighted')\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3aea6ce5-19ab-42e5-a946-87f4a3e2df00",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(results_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c1d2783b-25a0-42d6-a405-caf2e83f3f28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Vectorization</th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1-Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.808103</td>\n",
       "      <td>0.791542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Word2Vec</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.729742</td>\n",
       "      <td>0.691754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>Dense NN</td>\n",
       "      <td>0.812556</td>\n",
       "      <td>0.805390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Word2Vec</td>\n",
       "      <td>Dense NN</td>\n",
       "      <td>0.726625</td>\n",
       "      <td>0.690898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>Dense NN</td>\n",
       "      <td>0.807658</td>\n",
       "      <td>0.801577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Word2Vec</td>\n",
       "      <td>Dense NN</td>\n",
       "      <td>0.750668</td>\n",
       "      <td>0.726468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>Dense NN</td>\n",
       "      <td>0.807658</td>\n",
       "      <td>0.801239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Word2Vec</td>\n",
       "      <td>Dense NN</td>\n",
       "      <td>0.757792</td>\n",
       "      <td>0.737604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>RNN</td>\n",
       "      <td>0.809884</td>\n",
       "      <td>0.803681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Word2Vec</td>\n",
       "      <td>RNN</td>\n",
       "      <td>0.783170</td>\n",
       "      <td>0.778113</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Vectorization     Model  Accuracy  F1-Score\n",
       "0        TF-IDF       SVM  0.808103  0.791542\n",
       "1      Word2Vec       SVM  0.729742  0.691754\n",
       "2        TF-IDF  Dense NN  0.812556  0.805390\n",
       "3      Word2Vec  Dense NN  0.726625  0.690898\n",
       "4        TF-IDF  Dense NN  0.807658  0.801577\n",
       "5      Word2Vec  Dense NN  0.750668  0.726468\n",
       "6        TF-IDF  Dense NN  0.807658  0.801239\n",
       "7      Word2Vec  Dense NN  0.757792  0.737604\n",
       "8        TF-IDF       RNN  0.809884  0.803681\n",
       "9      Word2Vec       RNN  0.783170  0.778113"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
