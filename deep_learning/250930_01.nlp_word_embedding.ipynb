{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4844f9f1-b926-4899-a099-ae59c3148faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "임포트 완료\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from konlpy.tag import Okt\n",
    "from collections import Counter\n",
    "print(\"임포트 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "24197291-9c96-42da-9469-e059b08797c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'임금님 귀는 당나귀 귀! 임금님 귀는 당나귀 귀! 실컷~ 소리치고 나니 속이 확 뚫려 살 것 같았어.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '임금님 귀는 당나귀 귀! 임금님 귀는 당나귀 귀! 실컷~ 소리치고 나니 속이 확 뚫려 살 것 같았어.'\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d6534afe-da3f-4686-b612-c1f5366f93cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "임금님 귀는 당나귀 귀 임금님 귀는 당나귀 귀 실컷 소리치고 나니 속이 확 뚫려 살 것 같았어\n"
     ]
    }
   ],
   "source": [
    "# 전처리 - 특수문자 제거\n",
    "reg = re.compile('[^ㄱ-ㅎㅏ-ㅣ가-힣 ]')\n",
    "text = reg.sub('', text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "207a579a-3150-4972-80f0-c600c203ec54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['임금님', '귀', '는', '당나귀', '귀', '임금님', '귀', '는', '당나귀', '귀', '실컷', '소리', '치고', '나니', '속이', '확', '뚫려', '살', '것', '같았어']\n"
     ]
    }
   ],
   "source": [
    "# 형태소 분석기를 통한 토큰화\n",
    "okt=Okt()\n",
    "tokens = okt.morphs(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c7dc8f26-c7cd-4847-b3cb-e4ff4b918c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'귀': 4, '임금님': 2, '는': 2, '당나귀': 2, '실컷': 1, '소리': 1, '치고': 1, '나니': 1, '속이': 1, '확': 1, '뚫려': 1, '살': 1, '것': 1, '같았어': 1})\n"
     ]
    }
   ],
   "source": [
    "# python Counter() 섭클레스 사용, 빈도를 세고 빈도가 높은 순서로 정렬\n",
    "vocab = Counter(tokens)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8fab1755-61aa-49e7-ba0f-511b87695c8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 빈도수 리턴\n",
    "vocab['임금님']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "11073d77-f7f7-4f6a-9eed-ff446903d82b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('귀', 4), ('임금님', 2), ('는', 2), ('당나귀', 2), ('실컷', 1)]\n"
     ]
    }
   ],
   "source": [
    "# 빈도수 상위 5개의 단어만 저장\n",
    "vocab_size = 5\n",
    "vocab = vocab.most_common(vocab_size) # 등장 빈도수가 높은 상위 5개의 단어만 저장\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d5a74c92-8c66-42ca-bf57-2dc60328b315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'귀': 1, '임금님': 2, '는': 3, '당나귀': 4, '실컷': 5}\n"
     ]
    }
   ],
   "source": [
    "# 상위 5개의 단어에 인덱스 부여\n",
    "word2idx={word[0] : index+1 for index, word in enumerate(vocab)}\n",
    "print(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0ce2335b-1827-499b-a331-bea1a65b24c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(word, word2index):\n",
    "       one_hot_vector = [0]*(len(word2index))\n",
    "       index = word2index[word]\n",
    "       one_hot_vector[index-1] = 1\n",
    "       return one_hot_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c624cd80-162d-4d32-a006-b4ce5b1afe4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 0, 0, 0]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encoding('임금님', word2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089c4d6d-4a58-410e-a136-93d7bd9d46c3",
   "metadata": {},
   "source": [
    "- keras를 통한 원-핫 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e4381c47-59c5-4cc8-a553-10ed21579560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "임포트 완료\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "print(\"임포트 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c95d00a8-81bf-4d68-a68a-d489a1937266",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['강아지', '고양이', '강아지'], ['애교', '고양이'], ['컴퓨터', '노트북']]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = [['강아지', '고양이', '강아지'],['애교', '고양이'], ['컴퓨터', '노트북']]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f92e9c7d-ff80-469e-a93f-778fd9fdf0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'강아지': 1, '고양이': 2, '애교': 3, '컴퓨터': 4, '노트북': 5}\n"
     ]
    }
   ],
   "source": [
    "t = Tokenizer()\n",
    "t.fit_on_texts(text)\n",
    "print(t.word_index) # 각 단어에 대한 인코딩 결과 출력."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e1f1b274-ecee-44ce-935c-9c126b875a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어장 크기 저장\n",
    "vocab_size = len(t.word_index) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6822d1c-806f-4451-aaca-7e62fb647478",
   "metadata": {},
   "source": [
    "- 0번 단어가 특별 토큰으로 단어장에 추가되는 경우가 많음\n",
    "- 0번은 패딩 토큰\n",
    "- 그래서 vocab_size에 +1 해줌 (0번 토큰을 만들고 비워둠)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "02e1f9eb-5a1a-4e3e-9fe5-53f2133395d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 1, 4]]\n"
     ]
    }
   ],
   "source": [
    "# 텍스트를 정수 시퀀스로 변환 (단어장에 속한 단어의 경우 정수로 변환)\n",
    "sub_text = ['강아지', '고양이', '강아지', '컴퓨터']\n",
    "encoded = t.texts_to_sequences([sub_text])\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "68d322f8-8d5b-45c8-885e-c73ae5237e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 1. 0. 0. 0.]\n",
      "  [0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 1. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "one_hot = to_categorical(encoded, num_classes = vocab_size)\n",
    "print(one_hot)\n",
    "# 0 1 2 3 4 5 클래스"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02eacb78-d553-4197-a808-c143e9c428ed",
   "metadata": {},
   "source": [
    "# Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e7ab5a-4c79-424e-a83b-676319d73748",
   "metadata": {},
   "source": [
    "## 1. Word2Vec\n",
    "- 분포가설\n",
    "    - 곁에 오는 단어들을 보면 그 단어를 알 수 있다.\n",
    "    - 비슷한 문맥에서 같이 등장하는 경향이 있는 단어들은 비슷한 의미를 가진다.\n",
    "- Continuous Bag of Words (CBoW)\n",
    "    - 중심 단어를 예측하기 위해 주변 단어를 입력으로 사용\n",
    "    - 예문: \"I like natural language processing\"\n",
    "        - {\"i\", \"like\", \"language\", \"processing\"}으로부터 \"natural\"을 예측\n",
    "        - 중심단어(center word): \"natural\"\n",
    "        - 주변단어(context word): 그외 예측에 사용되는 단어들\n",
    "        - 윈도우(window): 중심단어를 예측하기 위해 볼 주변 단위의 범위\n",
    "        - 데이터셋 구성 형태: **((주변1, 주변2), 중심)**\n",
    "            - ((like), I), ((I, natural), like), ((like, language), natural), ((natural, processing), language), ((language), processing)\n",
    "    - Out-of-vocabulary(단어 집합에 없는 단어), Polysemy(다의어), 문맥파악의 한계점 존재\n",
    "- Skip-gram\n",
    "    - 중심 단어를 예측하기 위해 주변 단어를 입력으로 사용(CBoW와 메커니즘 자체는 동일)\n",
    "    - 데이터셋 구성 형태: **(중심, 주변)**\n",
    "        - (i, like) (like, I), (like, natural), (natural, like), (natural, language), (language, natural), (language, processing), (processing, language)\n",
    "- Negative sampling\n",
    "    - 대체적으로 SGNS(Skip-Gram with Negative Sampling)을 사용.\n",
    "    - 중심단어와 주변단어를 동시에 입력받아 두 단어가 정말 이웃 관계면 1 또는 0을 출력\n",
    "    - 예문: \"Thou shalt not make a machine in the likeness of a human mind\"\n",
    "| Input   | Output  | Target  |\n",
    "| -------- | -------- | -------- |\n",
    "| not      | Thou     | 1     |\n",
    "| not      | shalt    | 1     |\n",
    "| not      | make     | 1     |\n",
    "| not      | a        | 1     |\n",
    "| make     | shalt    | 1     |\n",
    "| make     | not      | 1     |\n",
    "| make     | a        | 1     |\n",
    "| make     | machine  | 1     |\n",
    "| .......  |  ....... | .......  |\n",
    "\n",
    "    - 랜덤으로 단어장에 있는 아무 단어나 가져와 target word로 하는 거짓 데이터셋을 만들어 0으로 레이블\n",
    "    - 이진 문제로 간주 - 출력층의 시그모이드 함수 - 레이블로부터 오차 역전파"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f67b59-fb6d-4433-beba-732f7de7ab9e",
   "metadata": {},
   "source": [
    "### 1-1. Word2Vec Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "529570ad-2efc-45c4-b597-13bd8880e659",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package abc to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package abc is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('abc')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ea657187-7771-420e-a3c7-667b1f01dae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import abc\n",
    "corpus = abc.sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d35fc95a-2118-49a6-aa53-5812dea3145d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['PM', 'denies', 'knowledge', 'of', 'AWB', 'kickbacks', 'The', 'Prime', 'Minister', 'has', 'denied', 'he', 'knew', 'AWB', 'was', 'paying', 'kickbacks', 'to', 'Iraq', 'despite', 'writing', 'to', 'the', 'wheat', 'exporter', 'asking', 'to', 'be', 'kept', 'fully', 'informed', 'on', 'Iraq', 'wheat', 'sales', '.'], ['Letters', 'from', 'John', 'Howard', 'and', 'Deputy', 'Prime', 'Minister', 'Mark', 'Vaile', 'to', 'AWB', 'have', 'been', 'released', 'by', 'the', 'Cole', 'inquiry', 'into', 'the', 'oil', 'for', 'food', 'program', '.'], ['In', 'one', 'of', 'the', 'letters', 'Mr', 'Howard', 'asks', 'AWB', 'managing', 'director', 'Andrew', 'Lindberg', 'to', 'remain', 'in', 'close', 'contact', 'with', 'the', 'Government', 'on', 'Iraq', 'wheat', 'sales', '.']]\n"
     ]
    }
   ],
   "source": [
    "print(corpus[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "845dfd91-d7b2-40ca-8f23-b4afb93f193c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "코퍼스의 크기 : 29059\n"
     ]
    }
   ],
   "source": [
    "print('코퍼스의 크기 :',len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7481a311-1020-467d-a220-cfc7d13f06a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 학습 완료!\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(sentences = corpus, vector_size = 100, window = 5, min_count = 5, workers = 4, sg = 0)\n",
    "print('모델 학습 완료!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e9d5d5-4ba4-4913-9d86-8d0cd6c65d59",
   "metadata": {},
   "source": [
    "- vector_size: 학습 후 임베딩 벡터 차원\n",
    "- window: 컨텍스트 윈도우 크기\n",
    "- min_count: 단어 최소 빈도수 제한 (빈도 적은 단어들은 학습에서 제외)\n",
    "- workers: 학습을 위한 프로세스 수\n",
    "- sg: 0(CBow), 1(Skip-gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e1a36114-5324-4f26-adf0-f06fa5be357e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('woman', 0.9233394265174866), ('skull', 0.9110252261161804), ('Bang', 0.9056580066680908), ('asteroid', 0.9052032232284546), ('third', 0.9020134806632996), ('baby', 0.8994045853614807), ('dog', 0.898598849773407), ('bought', 0.8975046277046204), ('rally', 0.8912537097930908), ('disc', 0.8889098167419434)]\n"
     ]
    }
   ],
   "source": [
    "model_result = model.wv.most_similar('man')\n",
    "print(model_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a041f488-9e4d-4160-86f5-7f992b5d62d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from gensim.models import KeyedVectors\n",
    "# 모델 저장\n",
    "os.makedirs('./word_embedding/', exist_ok=True)\n",
    "model.wv.save_word2vec_format('./word_embedding/w2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d8814cf6-6e9c-47bc-acf3-b356b8c373d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 load 완료!\n"
     ]
    }
   ],
   "source": [
    "# 모델 불러오기\n",
    "loaded_model = KeyedVectors.load_word2vec_format('./word_embedding/w2v')\n",
    "print('모델 load 완료!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b41cf0dc-b68b-4713-adab-9840ed8322a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('woman', 0.9233394265174866), ('skull', 0.9110252261161804), ('Bang', 0.9056580066680908), ('asteroid', 0.9052032232284546), ('third', 0.9020134806632996), ('baby', 0.8994045853614807), ('dog', 0.898598849773407), ('bought', 0.8975046277046204), ('rally', 0.8912537097930908), ('disc', 0.8889098167419434)]\n"
     ]
    }
   ],
   "source": [
    "model_result = loaded_model.most_similar('man')\n",
    "print(model_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945ced70-999c-4db0-a9f9-d234c2251495",
   "metadata": {},
   "source": [
    "#### Out of Vocabulary(OOV) 문제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ab3a8dea-7967-423e-9c0f-7c55f2961e84",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'overacting' not present in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 단어장에 없는 단어 입력시 에러\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mloaded_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmost_similar\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moveracting\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\tfgpuenv\\lib\\site-packages\\gensim\\models\\keyedvectors.py:841\u001b[0m, in \u001b[0;36mKeyedVectors.most_similar\u001b[1;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[0;32m    838\u001b[0m         weight[idx] \u001b[38;5;241m=\u001b[39m item[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    840\u001b[0m \u001b[38;5;66;03m# compute the weighted average of all keys\u001b[39;00m\n\u001b[1;32m--> 841\u001b[0m mean \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_mean_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre_normalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpost_normalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_missing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    842\u001b[0m all_keys \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    843\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_index(key) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m keys \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, _KEY_TYPES) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_index_for(key)\n\u001b[0;32m    844\u001b[0m ]\n\u001b[0;32m    846\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indexer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(topn, \u001b[38;5;28mint\u001b[39m):\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\tfgpuenv\\lib\\site-packages\\gensim\\models\\keyedvectors.py:518\u001b[0m, in \u001b[0;36mKeyedVectors.get_mean_vector\u001b[1;34m(self, keys, weights, pre_normalize, post_normalize, ignore_missing)\u001b[0m\n\u001b[0;32m    516\u001b[0m         total_weight \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mabs\u001b[39m(weights[idx])\n\u001b[0;32m    517\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ignore_missing:\n\u001b[1;32m--> 518\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not present in vocabulary\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    520\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m total_weight \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    521\u001b[0m     mean \u001b[38;5;241m=\u001b[39m mean \u001b[38;5;241m/\u001b[39m total_weight\n",
      "\u001b[1;31mKeyError\u001b[0m: \"Key 'overacting' not present in vocabulary\""
     ]
    }
   ],
   "source": [
    "# 단어장에 없는 단어 입력시 에러\n",
    "loaded_model.most_similar('overacting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5fa1b3c0-99df-432e-99d3-1d7498ef1470",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'memorry' not present in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# memory의 오타 가정\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mloaded_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmost_similar\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmemorry\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\tfgpuenv\\lib\\site-packages\\gensim\\models\\keyedvectors.py:841\u001b[0m, in \u001b[0;36mKeyedVectors.most_similar\u001b[1;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[0;32m    838\u001b[0m         weight[idx] \u001b[38;5;241m=\u001b[39m item[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    840\u001b[0m \u001b[38;5;66;03m# compute the weighted average of all keys\u001b[39;00m\n\u001b[1;32m--> 841\u001b[0m mean \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_mean_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre_normalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpost_normalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_missing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    842\u001b[0m all_keys \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    843\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_index(key) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m keys \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, _KEY_TYPES) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_index_for(key)\n\u001b[0;32m    844\u001b[0m ]\n\u001b[0;32m    846\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indexer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(topn, \u001b[38;5;28mint\u001b[39m):\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\tfgpuenv\\lib\\site-packages\\gensim\\models\\keyedvectors.py:518\u001b[0m, in \u001b[0;36mKeyedVectors.get_mean_vector\u001b[1;34m(self, keys, weights, pre_normalize, post_normalize, ignore_missing)\u001b[0m\n\u001b[0;32m    516\u001b[0m         total_weight \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mabs\u001b[39m(weights[idx])\n\u001b[0;32m    517\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ignore_missing:\n\u001b[1;32m--> 518\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not present in vocabulary\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    520\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m total_weight \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    521\u001b[0m     mean \u001b[38;5;241m=\u001b[39m mean \u001b[38;5;241m/\u001b[39m total_weight\n",
      "\u001b[1;31mKeyError\u001b[0m: \"Key 'memorry' not present in vocabulary\""
     ]
    }
   ],
   "source": [
    "# memory의 오타 가정\n",
    "loaded_model.most_similar('memorry')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec232a4-9765-4a75-bdb0-106052bf67de",
   "metadata": {},
   "source": [
    "## 2. FastText\n",
    "- 단어 내부의 subwords들을 학습\n",
    "- 예문: \"partial\"\n",
    "    - n = 3, <pa, par, art, rti, tia, ial, al>, \\<partial>\n",
    "    - n = 3 ~ 6, <pa, par, art, rti, ita, ial, al>, <par, arti, rtia, tial, ial>, <part, ...중략... , \\<partial>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "905dde21-24af-4a86-8076-3eda6dc015e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText 학습 완료!\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import FastText\n",
    "fasttext_model = FastText(corpus, window=5, min_count=5, workers=4, sg=1)\n",
    "print('FastText 학습 완료!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c33e6cdc-f034-48ab-83c2-74fa6b1a796c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('extracting', 0.9394649267196655),\n",
       " ('resolving', 0.9356498718261719),\n",
       " ('fluctuating', 0.932230532169342),\n",
       " ('attracting', 0.9315232634544373),\n",
       " ('declining', 0.92936772108078),\n",
       " ('lifting', 0.9286537766456604),\n",
       " ('reflecting', 0.9280219674110413),\n",
       " ('boosting', 0.9263381361961365),\n",
       " ('mounting', 0.9263135194778442),\n",
       " ('contracting', 0.9252020716667175)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word2Vec에서 에러 발생했던 단어1\n",
    "fasttext_model.wv.most_similar('overacting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "136bb3f8-4eca-4aee-923a-1e4977dd2bb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('memory', 0.9533040523529053),\n",
       " ('mechanisms', 0.8768883347511292),\n",
       " ('mechanism', 0.8692528009414673),\n",
       " ('musical', 0.8585295081138611),\n",
       " ('music', 0.8582504391670227),\n",
       " ('intelligence', 0.8534806966781616),\n",
       " ('basic', 0.8524240255355835),\n",
       " ('intermediate', 0.8434807658195496),\n",
       " ('mechanical', 0.8363848328590393),\n",
       " ('intercourse', 0.8351597785949707)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word2Vec에서 에러 발생했던 단어2\n",
    "fasttext_model.wv.most_similar('memoryy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d84e52-2da7-4f24-b56f-1fde8c73a8a7",
   "metadata": {},
   "source": [
    "## 3. Glove\n",
    "- 중심 단어 벡터와 주변 단어 벡터의 내적이 전체 코퍼스에서의 동시 등장 빈도의 로그값이 되도록 만드는 것\n",
    "- 전체 코퍼스에서의 동시 등장 빈도의 로그값과 중심 단어 벡터와 주변 단어 벡터의 내적값의 차이가 최소화되도록 두 벡터의 값을 학습하는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4f49d8ea-4b1f-4c8e-b464-67fc67c9616a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=================---------------------------------] 35.5% 23.4/66.0MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 66.0/66.0MB downloaded\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('cat', 0.9218004941940308),\n",
       " ('dogs', 0.8513158559799194),\n",
       " ('horse', 0.7907583117485046),\n",
       " ('puppy', 0.7754921317100525),\n",
       " ('pet', 0.7724708318710327),\n",
       " ('rabbit', 0.7720813751220703),\n",
       " ('pig', 0.7490060925483704),\n",
       " ('snake', 0.7399188280105591),\n",
       " ('baby', 0.7395570278167725),\n",
       " ('bite', 0.7387937307357788)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "glove_model = api.load(\"glove-wiki-gigaword-50\")  # glove vectors 다운로드\n",
    "glove_model.most_similar(\"dog\")  # 'dog'과 비슷한 단어 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "67e8ed9c-880a-4567-9bd9-25bc63cc39c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('impudence', 0.7842012643814087),\n",
       " ('puerile', 0.7816032767295837),\n",
       " ('winningly', 0.7644237875938416),\n",
       " ('grossness', 0.7576098442077637),\n",
       " ('deconstructions', 0.748936653137207),\n",
       " ('over-the-top', 0.7460805177688599),\n",
       " ('buffoonery', 0.7460456490516663),\n",
       " ('impetuosity', 0.7415392398834229),\n",
       " ('sophomoric', 0.7369617819786072),\n",
       " ('zaniness', 0.7353196740150452)]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_model.most_similar('overacting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "19098993-73f0-4ac9-99e1-0e4309d6c889",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'memoryy' not present in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[65], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mglove_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmost_similar\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmemoryy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\tfgpuenv\\lib\\site-packages\\gensim\\models\\keyedvectors.py:841\u001b[0m, in \u001b[0;36mKeyedVectors.most_similar\u001b[1;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[0;32m    838\u001b[0m         weight[idx] \u001b[38;5;241m=\u001b[39m item[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    840\u001b[0m \u001b[38;5;66;03m# compute the weighted average of all keys\u001b[39;00m\n\u001b[1;32m--> 841\u001b[0m mean \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_mean_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre_normalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpost_normalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_missing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    842\u001b[0m all_keys \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    843\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_index(key) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m keys \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, _KEY_TYPES) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_index_for(key)\n\u001b[0;32m    844\u001b[0m ]\n\u001b[0;32m    846\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indexer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(topn, \u001b[38;5;28mint\u001b[39m):\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\tfgpuenv\\lib\\site-packages\\gensim\\models\\keyedvectors.py:518\u001b[0m, in \u001b[0;36mKeyedVectors.get_mean_vector\u001b[1;34m(self, keys, weights, pre_normalize, post_normalize, ignore_missing)\u001b[0m\n\u001b[0;32m    516\u001b[0m         total_weight \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mabs\u001b[39m(weights[idx])\n\u001b[0;32m    517\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ignore_missing:\n\u001b[1;32m--> 518\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not present in vocabulary\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    520\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m total_weight \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    521\u001b[0m     mean \u001b[38;5;241m=\u001b[39m mean \u001b[38;5;241m/\u001b[39m total_weight\n",
      "\u001b[1;31mKeyError\u001b[0m: \"Key 'memoryy' not present in vocabulary\""
     ]
    }
   ],
   "source": [
    "glove_model.most_similar('memoryy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207a1cb1-7430-4aac-bc9e-a90ccab5f827",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
