{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "843cebf8-ef7a-45ee-a1ba-9db2feb0ce5f",
   "metadata": {},
   "source": [
    "# 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9144b224-4ae2-46c4-adcf-44b99936ce83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa12ac0e-efaa-4c87-910f-8b76613ad607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 239189\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "      <th>cc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>228238</th>\n",
       "      <td>He banged his head against a shelf and got a b...</td>\n",
       "      <td>Il s'est cogné la tête contre une étagère et s...</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157411</th>\n",
       "      <td>I promise you that I'll help you.</td>\n",
       "      <td>Je promets que je t'aiderai.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15914</th>\n",
       "      <td>I misjudged Tom.</td>\n",
       "      <td>Je me suis trompé sur Tom.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79863</th>\n",
       "      <td>This house is abandoned.</td>\n",
       "      <td>Cette demeure est abandonnée.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53754</th>\n",
       "      <td>Those houses are big.</td>\n",
       "      <td>Ces maisons sont grandes.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #6...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      eng  \\\n",
       "228238  He banged his head against a shelf and got a b...   \n",
       "157411                  I promise you that I'll help you.   \n",
       "15914                                    I misjudged Tom.   \n",
       "79863                            This house is abandoned.   \n",
       "53754                               Those houses are big.   \n",
       "\n",
       "                                                      fra  \\\n",
       "228238  Il s'est cogné la tête contre une étagère et s...   \n",
       "157411                       Je promets que je t'aiderai.   \n",
       "15914                          Je me suis trompé sur Tom.   \n",
       "79863                       Cette demeure est abandonnée.   \n",
       "53754                           Ces maisons sont grandes.   \n",
       "\n",
       "                                                       cc  \n",
       "228238  CC-BY 2.0 (France) Attribution: tatoeba.org #3...  \n",
       "157411  CC-BY 2.0 (France) Attribution: tatoeba.org #7...  \n",
       "15914   CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "79863   CC-BY 2.0 (France) Attribution: tatoeba.org #3...  \n",
       "53754   CC-BY 2.0 (France) Attribution: tatoeba.org #6...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = '../../data/translator_seq2seq/fra.txt'\n",
    "lines = pd.read_csv(file_path, names=['eng', 'fra', 'cc'], sep='\\t')\n",
    "print('전체 샘플의 수 :',len(lines))\n",
    "lines.sample(5) #샘플 5개 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73c3882e-76d9-4f88-a256-5987d549afe9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Va !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Marche.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Go.</td>\n",
       "      <td>En route !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Bouge !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Salut !</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   eng         fra\n",
       "0  Go.        Va !\n",
       "1  Go.     Marche.\n",
       "2  Go.  En route !\n",
       "3  Go.     Bouge !\n",
       "4  Hi.     Salut !"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = lines[['eng', 'fra']][:50000] # 5만개 샘플 사용\n",
    "lines[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c6f2a47-17e2-4533-9f0b-472abe2aea89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 50000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34856</th>\n",
       "      <td>I spilled my drink.</td>\n",
       "      <td>\\t J'ai renversé ma boisson. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29651</th>\n",
       "      <td>Take off your hat.</td>\n",
       "      <td>\\t Ôtez votre chapeau ! \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19294</th>\n",
       "      <td>You're creative.</td>\n",
       "      <td>\\t Vous êtes créative. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16872</th>\n",
       "      <td>It's dirt cheap.</td>\n",
       "      <td>\\t C'est tellement bon marché que c'en est ind...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7951</th>\n",
       "      <td>I got married.</td>\n",
       "      <td>\\t Je me suis marié. \\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       eng                                                fra\n",
       "34856  I spilled my drink.                    \\t J'ai renversé ma boisson. \\n\n",
       "29651   Take off your hat.                         \\t Ôtez votre chapeau ! \\n\n",
       "19294     You're creative.                          \\t Vous êtes créative. \\n\n",
       "16872     It's dirt cheap.  \\t C'est tellement bon marché que c'en est ind...\n",
       "7951        I got married.                            \\t Je me suis marié. \\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 시작 토큰과 종료 토큰 추가\n",
    "sos_token = '\\t'\n",
    "eos_token = '\\n'\n",
    "lines.fra = lines.fra.apply(lambda x : '\\t '+ x + ' \\n')\n",
    "print('전체 샘플의 수 :',len(lines))\n",
    "lines.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab205429-fd3b-42ba-b22b-4d9a23757bcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[19, 4, 7], [19, 4, 7], [19, 4, 7], [19, 4, 7], [11, 5, 7]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_tokenizer = Tokenizer(char_level=True)   # 문자 단위로 Tokenizer를 생성합니다. \n",
    "eng_tokenizer.fit_on_texts(lines.eng)        # 50000개의 행을 가진 eng의 각 행에 토큰화를 수행\n",
    "input_text = eng_tokenizer.texts_to_sequences(lines.eng)    # 단어를 숫자값 인덱스로 변환하여 저장\n",
    "input_text[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "458695e4-2950-46d5-9102-9ecb6a1a57d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[10, 1, 19, 5, 1, 31, 1, 11],\n",
       " [10, 1, 15, 5, 12, 16, 29, 2, 13, 1, 11],\n",
       " [10, 1, 2, 7, 1, 12, 9, 8, 4, 2, 1, 31, 1, 11],\n",
       " [10, 1, 26, 9, 8, 28, 2, 1, 31, 1, 11],\n",
       " [10, 1, 3, 5, 14, 8, 4, 1, 31, 1, 11]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fra_tokenizer = Tokenizer(char_level=True)   # 문자 단위로 Tokenizer를 생성합니다. \n",
    "fra_tokenizer.fit_on_texts(lines.fra)                 # 50000개의 행을 가진 fra의 각 행에 토큰화를 수행\n",
    "target_text = fra_tokenizer.texts_to_sequences(lines.fra)     # 단어를 숫자값 인덱스로 변환하여 저장\n",
    "target_text[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c329400-2f7d-4a61-ac07-c02133a6d55a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 단어장의 크기 : 53\n",
      "프랑스어 단어장의 크기 : 72\n"
     ]
    }
   ],
   "source": [
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "fra_vocab_size = len(fra_tokenizer.word_index) + 1\n",
    "print('영어 단어장의 크기 :', eng_vocab_size)\n",
    "print('프랑스어 단어장의 크기 :', fra_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3ab9659-95f7-4a30-9cf5-603a01d5305f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 시퀀스의 최대 길이 21\n",
      "프랑스어 시퀀스의 최대 길이 69\n"
     ]
    }
   ],
   "source": [
    "max_eng_seq_len = max([len(line) for line in input_text])\n",
    "max_fra_seq_len = max([len(line) for line in target_text])\n",
    "print('영어 시퀀스의 최대 길이', max_eng_seq_len)\n",
    "print('프랑스어 시퀀스의 최대 길이', max_fra_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc2e4d47-731e-45cd-b6e5-52f2e52d72f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 50000\n",
      "영어 단어장의 크기 : 53\n",
      "프랑스어 단어장의 크기 : 72\n",
      "영어 시퀀스의 최대 길이 21\n",
      "프랑스어 시퀀스의 최대 길이 69\n"
     ]
    }
   ],
   "source": [
    "print('전체 샘플의 수 :',len(lines))\n",
    "print('영어 단어장의 크기 :', eng_vocab_size)\n",
    "print('프랑스어 단어장의 크기 :', fra_vocab_size)\n",
    "print('영어 시퀀스의 최대 길이', max_eng_seq_len)\n",
    "print('프랑스어 시퀀스의 최대 길이', max_fra_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89822fcd-04b8-4125-bfbf-9680c062872e",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = input_text\n",
    "# 종료 토큰 제거\n",
    "decoder_input = [[ char for char in line if char != fra_tokenizer.word_index[eos_token] ] for line in target_text] \n",
    "# 시작 토큰 제거\n",
    "decoder_target = [[ char for char in line if char != fra_tokenizer.word_index[sos_token] ] for line in target_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12c8f2d9-f408-4913-bf15-58f6da322f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10, 1, 19, 5, 1, 31, 1], [10, 1, 15, 5, 12, 16, 29, 2, 13, 1], [10, 1, 2, 7, 1, 12, 9, 8, 4, 2, 1, 31, 1]]\n",
      "[[1, 19, 5, 1, 31, 1, 11], [1, 15, 5, 12, 16, 29, 2, 13, 1, 11], [1, 2, 7, 1, 12, 9, 8, 4, 2, 1, 31, 1, 11]]\n"
     ]
    }
   ],
   "source": [
    "print(decoder_input[:3])\n",
    "print(decoder_target[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "955e17c9-e1c7-40e2-8b58-965ae006406e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 데이터의 크기(shape) : (50000, 21)\n",
      "프랑스어 입력데이터의 크기(shape) : (50000, 69)\n",
      "프랑스어 출력데이터의 크기(shape) : (50000, 69)\n"
     ]
    }
   ],
   "source": [
    "encoder_input = pad_sequences(encoder_input, maxlen = max_eng_seq_len, padding='post')\n",
    "decoder_input = pad_sequences(decoder_input, maxlen = max_fra_seq_len, padding='post')\n",
    "decoder_target = pad_sequences(decoder_target, maxlen = max_fra_seq_len, padding='post')\n",
    "print('영어 데이터의 크기(shape) :',np.shape(encoder_input))\n",
    "print('프랑스어 입력데이터의 크기(shape) :',np.shape(decoder_input))\n",
    "print('프랑스어 출력데이터의 크기(shape) :',np.shape(decoder_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f85f58-386e-4938-b781-758d9def3932",
   "metadata": {},
   "source": [
    "- padding='post' => 뒤에 0으로 패딩한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf45083-b491-4be8-bcec-4142d8e62c39",
   "metadata": {},
   "source": [
    "- 데이터 shape: (샘플수, 샘플길이)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24dce8b6-59bf-48af-a41d-24e2a7064737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19  4  7  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "print(encoder_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e89d20b-b9e3-4d9f-945b-db1b69135326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 데이터의 크기(shape) : (50000, 21, 53)\n",
      "프랑스어 입력데이터의 크기(shape) : (50000, 69, 72)\n",
      "프랑스어 출력데이터의 크기(shape) : (50000, 69, 72)\n"
     ]
    }
   ],
   "source": [
    "# 원핫인코딩\n",
    "encoder_input = to_categorical(encoder_input)\n",
    "decoder_input = to_categorical(decoder_input)\n",
    "decoder_target = to_categorical(decoder_target)\n",
    "print('영어 데이터의 크기(shape) :',np.shape(encoder_input))\n",
    "print('프랑스어 입력데이터의 크기(shape) :',np.shape(decoder_input))\n",
    "print('프랑스어 출력데이터의 크기(shape) :',np.shape(decoder_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fab008d0-e5de-4dfc-a6e7-8a46bc6b5e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 학습데이터의 크기(shape) : (50000, 21, 53)\n",
      "프랑스어 학습 입력데이터의 크기(shape) : (50000, 69, 72)\n",
      "프랑스어 학습 출력데이터의 크기(shape) : (50000, 69, 72)\n"
     ]
    }
   ],
   "source": [
    "n_of_val = 3000\n",
    "\n",
    "encoder_input_train = encoder_input[:-n_of_val]\n",
    "decoder_input_train = decoder_input[:-n_of_val]\n",
    "decoder_target_train = decoder_target[:-n_of_val]\n",
    "\n",
    "encoder_input_test = encoder_input[-n_of_val:]\n",
    "decoder_input_test = decoder_input[-n_of_val:]\n",
    "decoder_target_test = decoder_target[-n_of_val:]\n",
    "\n",
    "print('영어 학습데이터의 크기(shape) :',np.shape(encoder_input))\n",
    "print('프랑스어 학습 입력데이터의 크기(shape) :',np.shape(decoder_input))\n",
    "print('프랑스어 학습 출력데이터의 크기(shape) :',np.shape(decoder_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0be4a7-f065-4b55-a08e-fe38da8226af",
   "metadata": {},
   "source": [
    "# 모델 훈련하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5fc4cfcd-003c-46b6-b951-527a2397f5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b079d7b-ad03-4211-b79c-20673a1b4620",
   "metadata": {},
   "source": [
    "## 인코더 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d9e9927-b13e-4936-824c-13069cde4de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 텐서 생성.\n",
    "encoder_inputs = Input(shape=(None, eng_vocab_size))\n",
    "# hidden size가 256인 인코더의 LSTM 셀 생성\n",
    "encoder_lstm = LSTM(units = 256, return_state = True)\n",
    "# 디코더로 전달할 hidden state, cell state를 리턴. encoder_outputs은 여기서는 불필요.\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "# hidden state와 cell state를 다음 time step으로 전달하기 위해서 별도 저장.\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b76933c-4873-444c-a9eb-788a91e7ede7",
   "metadata": {},
   "source": [
    "- LSTM 입력텐서 정의: 입력 문장을 저장하게 될 텐서\n",
    "- 256의 hidden state를 가지는 LSTM셀 생성, return_state=True를 통해 hidden state와 cell state를 리턴 받음\n",
    "    - 마지막 time step의 state_h와 state_c만 필요하므로 h를 모두 저장할 필요 없음\n",
    "    - 문장을 다 읽고 마지막만 전달 하면 됨. \n",
    "- 입력 텐서를 사용하여 마지막 hidden state 와 cell state를 결과 로 받음\n",
    "- 마지막 hidden state와 cell state를 하나의 변수에 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ff9690-2a26-49e6-86d0-26e32393b97c",
   "metadata": {},
   "source": [
    "# 디코더 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1625b981-d38f-47e1-b935-73bb56fa5226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 텐서 생성.\n",
    "decoder_inputs = Input(shape=(None, fra_vocab_size))\n",
    "# hidden size가 256인 디코더의 LSTM 셀 생성\n",
    "decoder_lstm = LSTM(units = 256, return_sequences = True, return_state=True)\n",
    "# decoder_outputs는 모든 time step의 hidden state\n",
    "decoder_outputs, _, _= decoder_lstm(decoder_inputs, initial_state = encoder_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c17728-eaa0-4420-91f2-974c8eb4330e",
   "metadata": {},
   "source": [
    "- initial_state = encoder_states (encoder의 마지막 h_T, c_T)\n",
    "    - 디코더야 너는 이 문장을 다 읽은 상태(h_T, c_T)에서부터 문장을 생성해. 같은 의미"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "38671ee4-9035-4e5f-9dac-dcbd24c34ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_softmax_layer = Dense(fra_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b2e26402-31ca-4800-a115-e8fd7e8f15f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, None, 53)]   0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, None, 72)]   0           []                               \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    [(None, 256),        317440      ['input_1[0][0]']                \n",
      "                                 (None, 256),                                                     \n",
      "                                 (None, 256)]                                                     \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  [(None, None, 256),  336896      ['input_2[0][0]',                \n",
      "                                 (None, 256),                     'lstm[0][1]',                   \n",
      "                                 (None, 256)]                     'lstm[0][2]']                   \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, None, 72)     18504       ['lstm_1[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 672,840\n",
      "Trainable params: 672,840\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8d0873c6-1f32-43c5-afcc-938a5b070147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "368/368 [==============================] - 287s 29ms/step - loss: 0.9655 - val_loss: 0.8162\n",
      "Epoch 2/50\n",
      "368/368 [==============================] - 8s 23ms/step - loss: 0.5781 - val_loss: 0.6503\n",
      "Epoch 3/50\n",
      "368/368 [==============================] - 9s 24ms/step - loss: 0.4800 - val_loss: 0.5816\n",
      "Epoch 4/50\n",
      "368/368 [==============================] - 10s 28ms/step - loss: 0.4208 - val_loss: 0.5169\n",
      "Epoch 5/50\n",
      "368/368 [==============================] - 10s 27ms/step - loss: 0.3817 - val_loss: 0.4873\n",
      "Epoch 6/50\n",
      "368/368 [==============================] - 10s 27ms/step - loss: 0.3541 - val_loss: 0.4616\n",
      "Epoch 7/50\n",
      "368/368 [==============================] - 10s 27ms/step - loss: 0.3324 - val_loss: 0.4304\n",
      "Epoch 8/50\n",
      "368/368 [==============================] - 10s 27ms/step - loss: 0.3154 - val_loss: 0.4247\n",
      "Epoch 9/50\n",
      "368/368 [==============================] - 10s 27ms/step - loss: 0.3010 - val_loss: 0.4084\n",
      "Epoch 10/50\n",
      "368/368 [==============================] - 10s 27ms/step - loss: 0.2887 - val_loss: 0.3982\n",
      "Epoch 11/50\n",
      "368/368 [==============================] - 10s 27ms/step - loss: 0.2788 - val_loss: 0.3970\n",
      "Epoch 12/50\n",
      "368/368 [==============================] - 10s 27ms/step - loss: 0.2703 - val_loss: 0.3935\n",
      "Epoch 13/50\n",
      "368/368 [==============================] - 10s 28ms/step - loss: 0.2617 - val_loss: 0.3856\n",
      "Epoch 14/50\n",
      "368/368 [==============================] - 10s 27ms/step - loss: 0.2543 - val_loss: 0.3790\n",
      "Epoch 15/50\n",
      "368/368 [==============================] - 10s 27ms/step - loss: 0.2476 - val_loss: 0.3772\n",
      "Epoch 16/50\n",
      "368/368 [==============================] - 10s 28ms/step - loss: 0.2414 - val_loss: 0.3784\n",
      "Epoch 17/50\n",
      "368/368 [==============================] - 10s 27ms/step - loss: 0.2357 - val_loss: 0.3734\n",
      "Epoch 18/50\n",
      "368/368 [==============================] - 10s 28ms/step - loss: 0.2304 - val_loss: 0.3677\n",
      "Epoch 19/50\n",
      "368/368 [==============================] - 10s 28ms/step - loss: 0.2255 - val_loss: 0.3685\n",
      "Epoch 20/50\n",
      "368/368 [==============================] - 10s 28ms/step - loss: 0.2214 - val_loss: 0.3686\n",
      "Epoch 21/50\n",
      "368/368 [==============================] - 10s 28ms/step - loss: 0.2167 - val_loss: 0.3709\n",
      "Epoch 22/50\n",
      "368/368 [==============================] - 10s 28ms/step - loss: 0.2126 - val_loss: 0.3680\n",
      "Epoch 23/50\n",
      "368/368 [==============================] - 10s 27ms/step - loss: 0.2087 - val_loss: 0.3720\n",
      "Epoch 24/50\n",
      "368/368 [==============================] - 10s 27ms/step - loss: 0.2049 - val_loss: 0.3664\n",
      "Epoch 25/50\n",
      "368/368 [==============================] - 10s 28ms/step - loss: 0.2013 - val_loss: 0.3735\n",
      "Epoch 26/50\n",
      "368/368 [==============================] - 10s 28ms/step - loss: 0.1981 - val_loss: 0.3691\n",
      "Epoch 27/50\n",
      "368/368 [==============================] - 10s 27ms/step - loss: 0.1947 - val_loss: 0.3730\n",
      "Epoch 28/50\n",
      "368/368 [==============================] - 10s 28ms/step - loss: 0.1916 - val_loss: 0.3743\n",
      "Epoch 29/50\n",
      "368/368 [==============================] - 10s 28ms/step - loss: 0.1884 - val_loss: 0.3762\n",
      "Epoch 30/50\n",
      "368/368 [==============================] - 10s 27ms/step - loss: 0.1854 - val_loss: 0.3779\n",
      "Epoch 31/50\n",
      "368/368 [==============================] - 10s 28ms/step - loss: 0.1826 - val_loss: 0.3857\n",
      "Epoch 32/50\n",
      "368/368 [==============================] - 10s 28ms/step - loss: 0.1798 - val_loss: 0.3770\n",
      "Epoch 33/50\n",
      "368/368 [==============================] - 10s 28ms/step - loss: 0.1772 - val_loss: 0.3822\n",
      "Epoch 34/50\n",
      "368/368 [==============================] - 10s 28ms/step - loss: 0.1746 - val_loss: 0.3893\n",
      "Epoch 35/50\n",
      "368/368 [==============================] - 10s 28ms/step - loss: 0.1721 - val_loss: 0.3857\n",
      "Epoch 36/50\n",
      "368/368 [==============================] - 11s 29ms/step - loss: 0.1697 - val_loss: 0.3913\n",
      "Epoch 37/50\n",
      "368/368 [==============================] - 10s 28ms/step - loss: 0.1674 - val_loss: 0.3951\n",
      "Epoch 38/50\n",
      "368/368 [==============================] - 11s 29ms/step - loss: 0.1651 - val_loss: 0.3971\n",
      "Epoch 39/50\n",
      "368/368 [==============================] - 10s 27ms/step - loss: 0.1630 - val_loss: 0.3972\n",
      "Epoch 40/50\n",
      "368/368 [==============================] - 10s 28ms/step - loss: 0.1607 - val_loss: 0.4009\n",
      "Epoch 41/50\n",
      "368/368 [==============================] - 10s 28ms/step - loss: 0.1587 - val_loss: 0.4033\n",
      "Epoch 42/50\n",
      "368/368 [==============================] - 10s 28ms/step - loss: 0.1567 - val_loss: 0.4082\n",
      "Epoch 43/50\n",
      "368/368 [==============================] - 10s 28ms/step - loss: 0.1547 - val_loss: 0.4062\n",
      "Epoch 44/50\n",
      "368/368 [==============================] - 10s 27ms/step - loss: 0.1529 - val_loss: 0.4101\n",
      "Epoch 45/50\n",
      "368/368 [==============================] - 10s 28ms/step - loss: 0.1510 - val_loss: 0.4159\n",
      "Epoch 46/50\n",
      "368/368 [==============================] - 10s 27ms/step - loss: 0.1492 - val_loss: 0.4155\n",
      "Epoch 47/50\n",
      "368/368 [==============================] - 10s 28ms/step - loss: 0.1474 - val_loss: 0.4254\n",
      "Epoch 48/50\n",
      "368/368 [==============================] - 10s 27ms/step - loss: 0.1458 - val_loss: 0.4249\n",
      "Epoch 49/50\n",
      "368/368 [==============================] - 10s 28ms/step - loss: 0.1440 - val_loss: 0.4261\n",
      "Epoch 50/50\n",
      "368/368 [==============================] - 8s 23ms/step - loss: 0.1424 - val_loss: 0.4278\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x245ca0370d0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train, \\\n",
    "          validation_data = ([encoder_input_test, decoder_input_test], decoder_target_test),\n",
    "          batch_size=128, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2311c8-777e-460c-84fa-f6d3017e5c08",
   "metadata": {},
   "source": [
    "# 모델 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf560382-dfc5-4041-8ea5-3ab9667c88f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, None, 53)]        0         \n",
      "                                                                 \n",
      " lstm (LSTM)                 [(None, 256),             317440    \n",
      "                              (None, 256),                       \n",
      "                              (None, 256)]                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 317,440\n",
      "Trainable params: 317,440\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_model = Model(inputs = encoder_inputs, outputs = encoder_states)\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "75f366c6-0572-41df-9dac-26c364bf37f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이전 time step의 hidden state를 저장하는 텐서\n",
    "decoder_state_input_h = Input(shape=(256,))\n",
    "# 이전 time step의 cell state를 저장하는 텐서\n",
    "decoder_state_input_c = Input(shape=(256,))\n",
    "# 이전 time step의 hidden state와 cell state를 하나의 변수에 저장\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "# decoder_states_inputs를 현재 time step의 초기 상태로 사용.\n",
    "# 구체적인 동작 자체는 def decode_sequence()에 구현.\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state = decoder_states_inputs)\n",
    "# 현재 time step의 hidden state와 cell state를 하나의 변수에 저장.\n",
    "decoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a868117e-1613-4c17-ac4e-7cd10c8a07eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, None, 72)]   0           []                               \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)           [(None, 256)]        0           []                               \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)           [(None, 256)]        0           []                               \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  [(None, None, 256),  336896      ['input_2[0][0]',                \n",
      "                                 (None, 256),                     'input_3[0][0]',                \n",
      "                                 (None, 256)]                     'input_4[0][0]']                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, None, 72)     18504       ['lstm_1[1][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 355,400\n",
      "Trainable params: 355,400\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
    "decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs, outputs=[decoder_outputs] + decoder_states)\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b89c5723-8253-4716-a6b8-4c356c51db21",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng2idx = eng_tokenizer.word_index\n",
    "fra2idx = fra_tokenizer.word_index\n",
    "idx2eng = eng_tokenizer.index_word\n",
    "idx2fra = fra_tokenizer.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e1221be3-8539-48de-a1cd-f4aec6db1171",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 상태를 얻음\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # <SOS>에 해당하는 원-핫 벡터 생성\n",
    "    target_seq = np.zeros((1, 1, fra_vocab_size))\n",
    "    target_seq[0, 0, fra2idx['\\t']] = 1.\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "\n",
    "    # stop_condition이 True가 될 때까지 루프 반복\n",
    "    while not stop_condition:\n",
    "        # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # 예측 결과를 문자로 변환\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = idx2fra[sampled_token_index]\n",
    "\n",
    "        # 현재 시점의 예측 문자를 예측 문장에 추가\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # <eos>에 도달하거나 최대 길이를 넘으면 중단.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_fra_seq_len):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
    "        target_seq = np.zeros((1, 1, fra_vocab_size))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97e6f3e-c041-401e-acc6-ed4f8dfca5c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
