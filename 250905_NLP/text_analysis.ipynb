{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9ad9263-13a4-4efb-b64c-2afa05a112de",
   "metadata": {},
   "source": [
    "# NLP(Natural Language Processing)와 TA(Text Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1dad5c-bd80-4a29-b5f7-7f813030d170",
   "metadata": {},
   "source": [
    "- NLP\n",
    "    - 머신이 인간의 언어를 이해하고 해석하는데 중점\n",
    "- TA\n",
    "    - 텍스트 마이닝(Text Mining) 이라고도 불림\n",
    "    - 비정형 텍스트에서 의미있는 정보를 추출하는데 중점\n",
    "- NLP는 TA발전의 기반기술\n",
    "    - NLP가 발전함에 따라 TA도 정교화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8db0f2-4824-46b6-81cc-20cf68f50911",
   "metadata": {},
   "source": [
    "# 텍스트 분석의 영역\n",
    "- 비지니스 인텔리전스(Business Intelligence)나 예측 분석\n",
    "    - 머신러닝, 언어 이해, 통계 등을 활용해 모델을 수립하고 정보를 추출\n",
    "- 기술 영역\n",
    "    - **텍스트 분류(Text Classification, Text Categorization) :** 문서가 속하는 카테고리 예측\n",
    "    - **감정분석 (Sentiment Analysis) :** 감정/판단/믿음/의견/기분 등 주관적 요소 분석\n",
    "    - **텍스트 요약 (Summarization) :** 주제나 중심사상 추출\n",
    "    - **텍스트 군집화(Text Clustering)와 유사도 측정 :** 비슷한 유형 문서를 군집화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247b0f20-ec67-483d-9876-834679b45d2e",
   "metadata": {},
   "source": [
    "# 텍스트 분석의 수행 프로세스"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdb4b06-443a-4b8e-9c60-38e42eb3b1f6",
   "metadata": {},
   "source": [
    "1. 텍스트 전처리\n",
    "2. 피처 벡터화/추출\n",
    "3. ML 모델 수립 및 학습/예측/평가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb826486-3998-4a03-9d98-771d7ea905e6",
   "metadata": {},
   "source": [
    "# 파이썬 기반의 NLP, 텍스트 분석 패키지"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8ce399-437c-4b35-8f1c-c4f7459eb64d",
   "metadata": {},
   "source": [
    "- **NLTK(Natural Language Toolkit for Python)**\n",
    "- **Gensim**\n",
    "- **SpaCy**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba517ad0-240b-4256-82cf-76c601e244df",
   "metadata": {},
   "source": [
    "# @@코드를 통한 실습@@"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8497d8e3-0533-4fc2-a533-0cf07d715f54",
   "metadata": {},
   "source": [
    "## 1. 텍스트 전처리 및 정규화\n",
    "- Cleansing\n",
    "- Tokenization\n",
    "- Filtering/Stop Word Removal\n",
    "- Stemming\n",
    "- Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2934bcbf-110c-4d69-ae60-ede8e7da6aeb",
   "metadata": {},
   "source": [
    "### 1-1. Cleansing\n",
    "- 분석에 방해가 되는 불필요한 문자, 기호 등을 사전에 제거\n",
    "- HTML, XML 태그나 특정 기호 등을 사전에 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c71a65-cc98-431b-bb96-3238c90e1b65",
   "metadata": {},
   "source": [
    "### 1-2. Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93b57f9-060e-4a26-b8ea-21d6ec7b4f6e",
   "metadata": {},
   "source": [
    "#### 1-2-1. Sentence Tokenization\n",
    "- 문장의 마침표(.), 개행문자(\\n) 등 문장의 마지막을 뜻하는 기호에 따라 분리\n",
    "- 또는 정규 표현식에 따른 문장 토큰화 가능\n",
    "- sent_tokenize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd4f3c3f-f2ef-4fda-a459-3f747adc8287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 3\n",
      "['The Matrix is everywhere its all around us, here even in this room.', 'You can see it out your window or on your television.', 'You feel it when you go to work, or go to church or pay your taxes.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "text_sample = 'The Matrix is everywhere its all around us, here even in this room. \\\n",
    "               You can see it out your window or on your television. \\\n",
    "               You feel it when you go to work, or go to church or pay your taxes.'\n",
    "sentences = sent_tokenize(text=text_sample)\n",
    "print(type(sentences), len(sentences))\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae18924-69bb-4966-834b-bcfe92f30d3f",
   "metadata": {},
   "source": [
    "#### 1-2-2. Word Tokenization\n",
    "- 문장을 단어로\n",
    "- 공백, 콤마(,), 마침표(.), 개행문자 등으로 분리\n",
    "- 또는 정규 표현식에 따른 단어 토큰화 가능\n",
    "- word_tokenize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec2fdc06-9d4c-409e-83d3-1e52328a6aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 15\n",
      "['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "sentence = \"The Matrix is everywhere its all around us, here even in this room.\"\n",
    "words = word_tokenize(sentence)\n",
    "print(type(words), len(words))\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedc22bc-e1f8-4ffc-8660-d491fb15682d",
   "metadata": {},
   "source": [
    "#### 1-2-3. Sentence Tokenization과 Word Tokenization 조합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf3b87b9-328f-49e9-9fce-9c6d5c041c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 3\n",
      "[['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.'], ['You', 'can', 'see', 'it', 'out', 'your', 'window', 'or', 'on', 'your', 'television', '.'], ['You', 'feel', 'it', 'when', 'you', 'go', 'to', 'work', ',', 'or', 'go', 'to', 'church', 'or', 'pay', 'your', 'taxes', '.']]\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, sent_tokenize\n",
    "\n",
    "# 문장별 단어 토큰화 함수 생성\n",
    "def tokenize_text(text):\n",
    "    \n",
    "    # 문장별 토큰\n",
    "    sentences = sent_tokenize(text)\n",
    "    # 문장별 단어별 토큰\n",
    "    word_tokens = [word_tokenize(sentence) for sentence in sentences]\n",
    "    return word_tokens\n",
    "\n",
    "# 토큰화 결과\n",
    "word_tokens = tokenize_text(text_sample)\n",
    "print(type(word_tokens), len(word_tokens))\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d220a09-118b-482d-a9cf-e1f3a7adfbe6",
   "metadata": {},
   "source": [
    "### 1-3. Stop Word Removal\n",
    "- 분석에 큰 의미 없는 단어 삭제\n",
    "- is, the, a, will 등 문맥적으로 큰 의미 없는 단어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "404128c6-56f8-4618-ac2d-689993834f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a487a4a-32ce-494c-96ac-bd731e1c4e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 stop words 갯수: 198\n",
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been']\n"
     ]
    }
   ],
   "source": [
    "print('영어 stop words 갯수:',len(nltk.corpus.stopwords.words('english')))\n",
    "print(nltk.corpus.stopwords.words('english')[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5eb4959b-a5ae-45ab-ab23-4d67b8889643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['matrix', 'everywhere', 'around', 'us', ',', 'even', 'room', '.'], ['see', 'window', 'television', '.'], ['feel', 'go', 'work', ',', 'go', 'church', 'pay', 'taxes', '.']]\n"
     ]
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "all_tokens = []\n",
    "# 문장별 word_tokens list 에 대해 stop word 제거\n",
    "for sentence in word_tokens:\n",
    "    filtered_words=[]\n",
    "    # 문장별 sentence list에 대해 stop word 제거\n",
    "    for word in sentence:\n",
    "        # 소문자 변환\n",
    "        word = word.lower()\n",
    "        # tokenize 된 개별 word가 stop words 들의 단어에 포함되지 않으면 word_tokens에 추가\n",
    "        if word not in stopwords:\n",
    "            filtered_words.append(word)\n",
    "    all_tokens.append(filtered_words)\n",
    "    \n",
    "print(all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b64833-7ff3-4acd-9233-59c307683b9c",
   "metadata": {},
   "source": [
    "### 1-4. Stemming 과 lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7aadf8-9027-4da8-8dc1-6a1ea56584e6",
   "metadata": {},
   "source": [
    "- 문법적 또는 의미적으로 변화하는 단어의 원형을 찾는 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "740a8dff-f7b0-4f63-9d55-03e48137b3e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work work work\n",
      "amus amus amus\n",
      "happy happiest\n",
      "fant fanciest\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "print(stemmer.stem('working'),stemmer.stem('works'),stemmer.stem('worked'))\n",
    "print(stemmer.stem('amusing'),stemmer.stem('amuses'),stemmer.stem('amused'))\n",
    "print(stemmer.stem('happier'),stemmer.stem('happiest'))\n",
    "print(stemmer.stem('fancier'),stemmer.stem('fanciest'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af47fb61-74b3-4295-8ca4-d371af057ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amuse amuse amuse\n",
      "happy happy\n",
      "fancy fancy\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "print(lemma.lemmatize('amusing','v'),lemma.lemmatize('amuses','v'),lemma.lemmatize('amused','v'))\n",
    "print(lemma.lemmatize('happier','a'),lemma.lemmatize('happiest','a'))\n",
    "print(lemma.lemmatize('fancier','a'),lemma.lemmatize('fanciest','a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca41bff-2a31-4a6d-aaf9-6f296d6e56d0",
   "metadata": {},
   "source": [
    "## 2. Bag of Words -BOW\n",
    "- 문서가 가진 모든 단어(words)를 문맥이나 순서를 무시하고 일괄적으로 빈도값을 부여해 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ce3003-79ba-4e48-90b1-6462a64dea34",
   "metadata": {},
   "source": [
    "### 2-1. 문서의 벡터화\n",
    "1. 카운트 기반의 벡터화\n",
    "2. TF-IDF(Term Frequency - Inverse Documant Frequency) 기반의 벡터화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feea1a54-b0f2-42c9-b43f-27db3fc91299",
   "metadata": {},
   "source": [
    "### 2-2. 사이킷런\n",
    "- **CountVectorizer**\n",
    "    - Count 벡터화 클래스\n",
    "    - 소문자 일괄 변환, 토큰화, 스톱워드 필터링 등 전처리도 함께 수행\n",
    "    - 피쳐 백터화 방법\n",
    "        1. 모든 문자를 소문자로 변경 등 전처리 수행\n",
    "        2. 디폴트로 단어 기준 n_gram_range 반영하여 토큰화\n",
    "        3. 텍스트 정규화 (단, stop_words 파라미터가 주어진 경우 스톱워드 필터링만 가능)\n",
    "        4. 어근 변환은 직접 지원하진 않지만 tokenizer 파라미터에 커스텀 어근 변환 함수를 적용 가능\n",
    "        5. max_df, min_df, max_feature등 파라미터 통해 토큰화된 단어를 피쳐로 추출하고 단어 빈도수 벡터값 적용\n",
    "- **TFidfVectorizer**\n",
    "    - TF-IDF 벡터화 클래스\n",
    "    -  파라미터와 변환방법은 CountVectorizer와 동일"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d351550-51af-48ef-b8b5-2afeb71c54d1",
   "metadata": {},
   "source": [
    "### 2-3. 희소행렬\n",
    "- 값이 대부분 0으로 이루어진 행렬\n",
    "- 메모리를 많이 차지함\n",
    "- 적은 메모리를 차지하도록 변환하는 방법 필요\n",
    "- **COO** 형식과 **CSR** 형식이 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bcecca-13d3-4d01-94c2-234f62568b96",
   "metadata": {},
   "source": [
    "#### 2-3-1. COO(Coordinate) 형식 개념\n",
    "- 0이 아닌 값과 좌표로 표현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "77d8d3fd-9826-4a1a-b656-5ea73fd9f0c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 0, 1],\n",
       "       [0, 2, 0]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 임의의 데이터 생성\n",
    "dense = np.array( [ [ 3, 0, 1 ], [0, 2, 0 ] ] )\n",
    "dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0bac7cce-f6e9-456e-a526-f5ab16d52488",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "# 0 이 아닌 데이터 추출\n",
    "data = np.array([3,1,2])\n",
    "\n",
    "# 행 위치와 열 위치를 각각 array로 생성 \n",
    "row_pos = np.array([0,0,1]) # 3(0, 0), 1(0, 2), 2(1, 1)\n",
    "col_pos = np.array([0,2,1])\n",
    "\n",
    "# sparse 패키지의 coo_matrix를 이용하여 COO 형식으로 희소 행렬 생성\n",
    "sparse_coo = sparse.coo_matrix((data, (row_pos,col_pos)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7454b76e-640e-4b26-8fba-6608f1e2a5ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 0, 1],\n",
       "       [0, 2, 0]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 행렬로 변환\n",
    "sparse_coo = sparse.coo_matrix((data, (row_pos, col_pos)))\n",
    "sparse_coo.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ae029c-b64f-49a4-bd34-7feb3fe04bea",
   "metadata": {},
   "source": [
    "#### 2-3-2. CSR(Compressed Sparse Row) 형식 개념\n",
    "- 행 위치의 시작점을 인덱싱 (같은 위치 값 반복하는것 개선)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c7f47c36-ffd1-4a67-831c-8f91240ad60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COO 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인\n",
      "[[0 0 1 0 0 5]\n",
      " [1 4 0 3 2 5]\n",
      " [0 6 0 3 0 0]\n",
      " [2 0 0 0 0 0]\n",
      " [0 0 0 7 0 8]\n",
      " [1 0 0 0 0 0]]\n",
      "CSR 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인\n",
      "[[0 0 1 0 0 5]\n",
      " [1 4 0 3 2 5]\n",
      " [0 6 0 3 0 0]\n",
      " [2 0 0 0 0 0]\n",
      " [0 0 0 7 0 8]\n",
      " [1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "dense2 = np.array([[0,0,1,0,0,5],\n",
    "             [1,4,0,3,2,5],\n",
    "             [0,6,0,3,0,0],\n",
    "             [2,0,0,0,0,0],\n",
    "             [0,0,0,7,0,8],\n",
    "             [1,0,0,0,0,0]])\n",
    "\n",
    "# 0 이 아닌 데이터 추출\n",
    "data2 = np.array([1, 5, 1, 4, 3, 2, 5, 6, 3, 2, 7, 8, 1])\n",
    "\n",
    "# 행 위치와 열 위치를 각각 array로 생성 \n",
    "row_pos = np.array([0, 0, 1, 1, 1, 1, 1, 2, 2, 3, 4, 4, 5])\n",
    "col_pos = np.array([2, 5, 0, 1, 3, 4, 5, 1, 3, 0, 3, 5, 0])\n",
    "\n",
    "# COO 형식으로 변환 \n",
    "sparse_coo = sparse.coo_matrix((data2, (row_pos,col_pos)))\n",
    "\n",
    "# 행 위치 배열의 고유한 값들의 시작 위치 인덱스를 배열로 생성\n",
    "row_pos_ind = np.array([0, 2, 7, 9, 10, 12, 13])\n",
    "\n",
    "# CSR 형식으로 변환 \n",
    "sparse_csr = sparse.csr_matrix((data2, col_pos, row_pos_ind))\n",
    "\n",
    "print('COO 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인')\n",
    "print(sparse_coo.toarray())\n",
    "print('CSR 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인')\n",
    "print(sparse_csr.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d417bc7-0e0b-4d48-8479-1b562bbcf3fe",
   "metadata": {},
   "source": [
    "#### 2-3-3. 사이킷런 COO및 CSR 실제 사용 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c08f2dcb-4477-4fd5-8f94-3b9b38a42449",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense3 = np.array([[0,0,1,0,0,5],\n",
    "             [1,4,0,3,2,5],\n",
    "             [0,6,0,3,0,0],\n",
    "             [2,0,0,0,0,0],\n",
    "             [0,0,0,7,0,8],\n",
    "             [1,0,0,0,0,0]])\n",
    "\n",
    "coo = sparse.coo_matrix(dense3)\n",
    "csr = sparse.csr_matrix(dense3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe19c9b-7f76-4b18-a8ac-a6353f911f9a",
   "metadata": {},
   "source": [
    "## 3. 텍스트 분류 실습 - 20 뉴스그룹 분류"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa181368-0a80-4486-98ba-95ae565010b9",
   "metadata": {},
   "source": [
    "## 3-1. 텍스트 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ff280758-eaa0-4762-8df4-e9a09b390804",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "news_data = fetch_20newsgroups(subset='all',random_state=156)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "de050198-18a6-4d24-837a-0cb2052b797a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])\n"
     ]
    }
   ],
   "source": [
    "print(news_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5290cada-79a2-41b7-923b-ddb62cbb9579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target 클래스의 값과 분포도\n",
      "0     799\n",
      "1     973\n",
      "2     985\n",
      "3     982\n",
      "4     963\n",
      "5     988\n",
      "6     975\n",
      "7     990\n",
      "8     996\n",
      "9     994\n",
      "10    999\n",
      "11    991\n",
      "12    984\n",
      "13    990\n",
      "14    987\n",
      "15    997\n",
      "16    910\n",
      "17    940\n",
      "18    775\n",
      "19    628\n",
      "Name: count, dtype: int64\n",
      "target 클래스의 이름들\n",
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print('target 클래스의 값과 분포도', pd.Series(news_data.target).value_counts().sort_index(), sep='\\n')\n",
    "print('target 클래스의 이름들', news_data.target_names, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "76baf927-d46d-4346-a58d-1f2e9fabfaef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: egreen@east.sun.com (Ed Green - Pixel Cruncher)\n",
      "Subject: Re: Observation re: helmets\n",
      "Organization: Sun Microsystems, RTP, NC\n",
      "Lines: 21\n",
      "Distribution: world\n",
      "Reply-To: egreen@east.sun.com\n",
      "NNTP-Posting-Host: laser.east.sun.com\n",
      "\n",
      "In article 211353@mavenry.altcit.eskimo.com, maven@mavenry.altcit.eskimo.com (Norman Hamer) writes:\n",
      "> \n",
      "> The question for the day is re: passenger helmets, if you don't know for \n",
      ">certain who's gonna ride with you (like say you meet them at a .... church \n",
      ">meeting, yeah, that's the ticket)... What are some guidelines? Should I just \n",
      ">pick up another shoei in my size to have a backup helmet (XL), or should I \n",
      ">maybe get an inexpensive one of a smaller size to accomodate my likely \n",
      ">passenger? \n",
      "\n",
      "If your primary concern is protecting the passenger in the event of a\n",
      "crash, have him or her fitted for a helmet that is their size.  If your\n",
      "primary concern is complying with stupid helmet laws, carry a real big\n",
      "spare (you can put a big or small head in a big helmet, but not in a\n",
      "small one).\n",
      "\n",
      "---\n",
      "Ed Green, former Ninjaite |I was drinking last night with a biker,\n",
      "  Ed.Green@East.Sun.COM   |and I showed him a picture of you.  I said,\n",
      "DoD #0111  (919)460-8302  |\"Go on, get to know her, you'll like her!\"\n",
      " (The Grateful Dead) -->  |It seemed like the least I could do...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(news_data.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362fd040-b3af-4aae-a4bf-3b833564e491",
   "metadata": {},
   "source": [
    "- 본문 제외 제목 등의 다른 정보 제거 (Target Leakage 방지)\n",
    "    - remove 파라미터 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cff07ccf-7688-404c-9dbb-c584540555af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터 크기 11314 , 테스트 데이터 크기 7532\n"
     ]
    }
   ],
   "source": [
    "# subset='train'으로 학습용(Train) 데이터만 추출, remove=('headers', 'footers', 'quotes')로 내용만 추출\n",
    "train_news= fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'), random_state=156)\n",
    "X_train = train_news.data\n",
    "y_train = train_news.target\n",
    "\n",
    "# subset='test'으로 테스트(Test) 데이터만 추출, remove=('headers', 'footers', 'quotes')로 내용만 추출\n",
    "test_news= fetch_20newsgroups(subset='test',remove=('headers', 'footers','quotes'),random_state=156)\n",
    "X_test = test_news.data\n",
    "y_test = test_news.target\n",
    "print('학습 데이터 크기 {0} , 테스트 데이터 크기 {1}'.format(len(train_news.data) , len(test_news.data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5a41f1-e3f0-4463-ab8c-98c5ee177d47",
   "metadata": {},
   "source": [
    "## 3-2. 피쳐 벡터화 및 머신러닝 모델 학습/예측/평가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b6bfef-bcf3-4f94-99f3-6d5b17f34499",
   "metadata": {},
   "source": [
    "### 3-2-1. Count 기반 피쳐 벡터화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3c1ab8a3-ecf7-49e8-86dd-65498035c11b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터 Text의 CountVectorizer Shape: (11314, 101631)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Count Vectorization으로 feature extraction 변환 수행. \n",
    "cnt_vect = CountVectorizer()\n",
    "\n",
    "# train 데이터 벡터화\n",
    "cnt_vect.fit(X_train)\n",
    "X_train_cnt_vect = cnt_vect.transform(X_train)\n",
    "\n",
    "# test 데이터 벡터화\n",
    "X_test_cnt_vect = cnt_vect.transform(X_test)\n",
    "\n",
    "print('학습 데이터 Text의 CountVectorizer Shape:',X_train_cnt_vect.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a457119-87c1-4708-a1d5-07bbbb630341",
   "metadata": {},
   "source": [
    "- Logistic Regression을 활용한 뉴스그룹 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d5a85a3d-0264-4ffb-915f-8a92f4789ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorized Logistic Regression 의 예측 정확도는 0.617\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# LogisticRegression을 이용하여 학습/예측/평가 수행. \n",
    "lr_clf = LogisticRegression(solver='liblinear')\n",
    "lr_clf.fit(X_train_cnt_vect , y_train)\n",
    "pred = lr_clf.predict(X_test_cnt_vect)\n",
    "print('CountVectorized Logistic Regression 의 예측 정확도는 {0:.3f}'.format(accuracy_score(y_test,pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca0844c-b11d-4efd-a7a9-1f320ffcdab2",
   "metadata": {},
   "source": [
    "### 3-2-2. TF-IDF 기반 피쳐 벡터화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1adb2b59-035a-4809-85f8-ce645bd15e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Logistic Regression 의 예측 정확도는 0.678\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# TF-IDF Vectorization 적용하여 학습 데이터셋과 테스트 데이터 셋 변환. \n",
    "tfidf_vect = TfidfVectorizer()\n",
    "tfidf_vect.fit(X_train)\n",
    "X_train_tfidf_vect = tfidf_vect.transform(X_train)\n",
    "X_test_tfidf_vect = tfidf_vect.transform(X_test)\n",
    "\n",
    "# LogisticRegression을 이용하여 학습/예측/평가 수행. \n",
    "lr_clf = LogisticRegression(solver='liblinear')\n",
    "lr_clf.fit(X_train_tfidf_vect , y_train)\n",
    "pred = lr_clf.predict(X_test_tfidf_vect)\n",
    "print('TF-IDF Logistic Regression 의 예측 정확도는 {0:.3f}'.format(accuracy_score(y_test ,pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f44a99d-bfa3-44d0-a7f1-eeac9afbc408",
   "metadata": {},
   "source": [
    "### 3-2-3. 성능향상\n",
    "- 최적의 ML 알고리즘 선택\n",
    "- 최상의 피쳐 전처리 수행"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf6d469-ae17-47dc-8763-9287eb820ba6",
   "metadata": {},
   "source": [
    "- 벡터화 파라미터 조정\n",
    "    - stop words 필터링을 추가\n",
    "    - ngram을 (1,2)로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d7ca2153-fec1-4a66-affa-5a54e7d1dbc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Vectorized Logistic Regression 의 예측 정확도는 0.690\n"
     ]
    }
   ],
   "source": [
    "# stop words 필터링을 추가하고 ngram을 기본(1,1)에서 (1,2)로 변경하여 Feature Vectorization 적용.\n",
    "tfidf_vect = TfidfVectorizer(stop_words='english', ngram_range=(1,2), max_df=300 )\n",
    "tfidf_vect.fit(X_train)\n",
    "X_train_tfidf_vect = tfidf_vect.transform(X_train)\n",
    "X_test_tfidf_vect = tfidf_vect.transform(X_test)\n",
    "\n",
    "lr_clf = LogisticRegression(solver='liblinear')\n",
    "lr_clf.fit(X_train_tfidf_vect , y_train)\n",
    "pred = lr_clf.predict(X_test_tfidf_vect)\n",
    "print('TF-IDF Vectorized Logistic Regression 의 예측 정확도는 {0:.3f}'.format(accuracy_score(y_test ,pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce21bde-d668-414d-9f3b-37f3cd5307ff",
   "metadata": {},
   "source": [
    "- 그리드 서치를 통한 로지스틱 리그레션 하이퍼파라미터 조정\n",
    "    - 규제 강도 조절"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b83a8c04-9ec4-4ad8-83e9-90dd2831b336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
      "Logistic Regression best C parameter : {'C': 10}\n",
      "TF-IDF Vectorized Logistic Regression 의 예측 정확도는 0.704\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 최적 C 값 도출 튜닝 수행. CV는 3 Fold셋으로 설정. \n",
    "params = { 'C':[0.01, 0.1, 1, 5, 10]}\n",
    "grid_cv_lr = GridSearchCV(lr_clf ,param_grid=params , cv=3 , scoring='accuracy' , verbose=1 )\n",
    "grid_cv_lr.fit(X_train_tfidf_vect , y_train)\n",
    "print('Logistic Regression best C parameter :',grid_cv_lr.best_params_ )\n",
    "\n",
    "# 최적 C 값으로 학습된 grid_cv로 예측 수행하고 정확도 평가. \n",
    "pred = grid_cv_lr.predict(X_test_tfidf_vect)\n",
    "print('TF-IDF Vectorized Logistic Regression 의 예측 정확도는 {0:.3f}'.format(accuracy_score(y_test ,pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b4ca64-1811-4d36-8a98-428c3a5f9aa0",
   "metadata": {},
   "source": [
    "### 3-2-4. 사이킷런 Pipeline 사용을 통한 GridSearchCV와의 결합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e8e8262f-4fe1-49fd-ad84-ca64b508afd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline을 통한 Logistic Regression 의 예측 정확도는 0.704\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# TfidfVectorizer 객체를 tfidf_vect 객체명으로, LogisticRegression객체를 lr_clf 객체명으로 생성하는 Pipeline생성\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf_vect', TfidfVectorizer(stop_words='english', ngram_range=(1,2), max_df=300)),\n",
    "    ('lr_clf', LogisticRegression(solver='liblinear', C=10))\n",
    "])\n",
    "\n",
    "# 별도의 TfidfVectorizer객체의 fit_transform( )과 LogisticRegression의 fit(), predict( )가 필요 없음. \n",
    "# pipeline의 fit( ) 과 predict( ) 만으로 한꺼번에 Feature Vectorization과 ML 학습/예측이 가능. \n",
    "pipeline.fit(X_train, y_train)\n",
    "pred = pipeline.predict(X_test)\n",
    "print('Pipeline을 통한 Logistic Regression 의 예측 정확도는 {0:.3f}'.format(accuracy_score(y_test ,pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb12382-5d27-4fe2-a40d-bb275235c170",
   "metadata": {},
   "source": [
    "- 다양한 파라미터 시도\n",
    "    - 연속 언더바2개(__)를 통해 파이프라인의 객체와 적용할 하이퍼파라미터 범위 설정 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "14cddbd5-f103-4084-a976-f0f6710d0a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n",
      "{'lr_clf__C': 10, 'tfidf_vect__max_df': 700, 'tfidf_vect__ngram_range': (1, 2)} 0.7550828826229531\n",
      "학습 소요 시간: 11.62 분\n",
      "Pipeline을 통한 Logistic Regression 의 예측 정확도는 0.702\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf_vect', TfidfVectorizer(stop_words='english')),    # 객체1의 이름 'tfidf_vect'\n",
    "    ('lr_clf', LogisticRegression(solver='liblinear'))        # 객체2의 이름 'lr_clf'\n",
    "])\n",
    "\n",
    "#시작 시간 기록\n",
    "start_time = datetime.now()\n",
    "\n",
    "# Pipeline에 기술된 각각의 객체 변수에 언더바(_)2개를 연달아 붙여 GridSearchCV에 사용될 \n",
    "# 파라미터/하이퍼 파라미터 이름과 값을 설정. . \n",
    "params = { 'tfidf_vect__ngram_range': [(1,1), (1,2), (1,3)],  # 객체1('tfidf_vect')의 ngram_range 서치 범위\n",
    "           'tfidf_vect__max_df': [100, 300, 700],             # 객체1('tfidf_vect')의 max_df 서치 범위\n",
    "           'lr_clf__C': [1, 5, 10]                            # 객체2('lr_clf')의 C 서치 범위\n",
    "}\n",
    "\n",
    "# GridSearchCV의 생성자에 Estimator가 아닌 Pipeline 객체 입력\n",
    "grid_cv_pipe = GridSearchCV(pipeline, param_grid=params, cv=3 , scoring='accuracy',verbose=1)\n",
    "grid_cv_pipe.fit(X_train , y_train)\n",
    "print(grid_cv_pipe.best_params_ , grid_cv_pipe.best_score_)\n",
    "\n",
    "# 끝 시간 기록\n",
    "end_time = datetime.now()\n",
    "\n",
    "# 소요 시간 계산\n",
    "elapsed_time = end_time - start_time\n",
    "elapsed_minutes = elapsed_time.total_seconds() / 60  # 초 → 분\n",
    "print(f\"학습 소요 시간: {elapsed_minutes:.2f} 분\")\n",
    "\n",
    "# 모델 예측\n",
    "pred = grid_cv_pipe.predict(X_test)\n",
    "print('Pipeline을 통한 Logistic Regression 의 예측 정확도는 {0:.3f}'.format(accuracy_score(y_test ,pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d23c18b-3bc8-47e8-b4d4-2b4def8558ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
